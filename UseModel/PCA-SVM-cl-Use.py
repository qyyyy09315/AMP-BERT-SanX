import pandas as pd
import numpy as np
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
import os

# -------------------------------
# 1. è¯»å–è®­ç»ƒé›† Parquet æ•°æ®
# -------------------------------
print("Loading training dataset from Parquet...")
df_train = pd.read_parquet("./train_em_cl.parquet")
print(f"Training dataset shape: {df_train.shape}")

# -------------------------------
# 2. ç¡®å®šæ ‡ç­¾åˆ—
# -------------------------------
label_column = "label"
if label_column not in df_train.columns:
    possible_labels = ['Label', 'class', 'Class', 'y']
    for col in possible_labels:
        if col in df_train.columns:
            label_column = col
            break
    else:
        raise ValueError(f"âŒ No label column found. Available: {df_train.columns}")

# -------------------------------
# 3. æå–ç‰¹å¾
# -------------------------------
feature_columns = [col for col in df_train.columns if col.startswith("embedding_dim_")]
if len(feature_columns) == 0:
    raise ValueError("âŒ No embedding feature columns found!")

X_train = df_train[feature_columns].values
y_train = df_train[label_column].values

print(f"Training Features shape: {X_train.shape}")
print(f"Labels shape: {y_train.shape}")
print(f"Unique classes: {np.unique(y_train)}")

# -------------------------------
# 4. ã€åŠ é€Ÿå…³é”®1ã€‘PCA é™ç»´ï¼ˆä¾‹å¦‚é™åˆ° 128 æˆ– 256 ç»´ï¼‰
# -------------------------------
n_components = 256  # å¯é€‰ï¼š64, 128, 256ï¼›æ¨è 128~256
print(f"\nApplying PCA to reduce dimension to {n_components}...")
pca = PCA(n_components=n_components, random_state=42)
X_train_pca = pca.fit_transform(X_train)

print(f"Reduced features shape: {X_train_pca.shape}")
# ä¿ç•™ 95%+ ä¿¡æ¯ï¼Ÿ
print(f"Explained variance ratio (cumulative): {pca.explained_variance_ratio_.sum():.4f}")

# -------------------------------
# 5. ã€åŠ é€Ÿå…³é”®2ã€‘æ ‡å‡†åŒ–ï¼ˆSVM å¿…éœ€ï¼‰
# -------------------------------
print("Standardizing features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_pca)
print("âœ… Preprocessing completed.")

# -------------------------------
# 6. ã€åŠ é€Ÿå…³é”®3ã€‘ä½¿ç”¨ SGDClassifierï¼ˆçº¿æ€§ SVMï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®ï¼‰
# -------------------------------
print("\nâœ… Training Linear SVM using SGD (fast, scalable)...")

svm_model = SGDClassifier(
    loss='hinge',           # ç›¸å½“äºçº¿æ€§ SVM
    alpha=0.0001,           # æ­£åˆ™åŒ–å¼ºåº¦ (1/C)
    max_iter=1000,          # å¯æ ¹æ®æ•°æ®é‡è°ƒæ•´
    tol=1e-4,
    random_state=42,
    n_jobs=-1,              # å¹¶è¡Œ
    learning_rate='optimal',
    verbose=0               # å¯è®¾ä¸º1æŸ¥çœ‹è®­ç»ƒè¿›åº¦
)

# è®­ç»ƒï¼ˆé€Ÿåº¦æå¿«ï¼‰
svm_model.fit(X_train_scaled, y_train)
print("âœ… Training completed.")

# -------------------------------
# 7. åŠ è½½æµ‹è¯•é›†å¹¶è¯„ä¼°
# -------------------------------
test_data_path = "./test_em_cl.parquet"
if not os.path.exists(test_data_path):
    raise FileNotFoundError(f"âŒ Test file not found: {test_data_path}")

print(f"\nLoading test dataset: {test_data_path}")
df_test = pd.read_parquet(test_data_path)

if label_column not in df_test.columns:
    raise ValueError(f"âŒ Label column '{label_column}' not found in test data.")

X_test = df_test[feature_columns].values
y_test = df_test[label_column].values

# åŒæ ·è¿›è¡Œ PCA + æ ‡å‡†åŒ–
X_test_pca = pca.transform(X_test)          # âš ï¸ åª transform
X_test_scaled = scaler.transform(X_test_pca) # âš ï¸ ä½¿ç”¨è®­ç»ƒé›† scaler

print(f"Test set shape after PCA: {X_test_scaled.shape}")

# -------------------------------
# 8. é¢„æµ‹ä¸è¯„ä¼°
# -------------------------------
print("\nEvaluating on test set...")
y_pred = svm_model.predict(X_test_scaled)

acc = accuracy_score(y_test, y_pred)
print(f"\nâœ… Test Accuracy: {acc:.4f}")
print("\nğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred))

# -------------------------------
# 9. æ··æ·†çŸ©é˜µå¯è§†åŒ–
# -------------------------------
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y_test),
            yticklabels=np.unique(y_test))
plt.title("Confusion Matrix - Test Set (Fast SVM)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.tight_layout()
plt.show()

# -------------------------------
# 10. ä¿å­˜æ‰€æœ‰ç»„ä»¶ï¼ˆæ¨¡å‹ + scaler + PCAï¼‰
# -------------------------------
model_save_dir = "../models"
os.makedirs(model_save_dir, exist_ok=True)

joblib.dump(svm_model, os.path.join(model_save_dir, "svm_fast_protein_classifier.pkl"))
joblib.dump(scaler, os.path.join(model_save_dir, "feature_scaler.pkl"))
joblib.dump(pca, os.path.join(model_save_dir, "pca_transformer.pkl"))

print(f"\nğŸ’¾ Model saved to '{model_save_dir}/svm_fast_protein_classifier.pkl'")
print(f"ğŸ’¾ Scaler saved.")
print(f"ğŸ’¾ PCA transformer saved.")