import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.preprocessing import QuantileTransformer
from sklearn.feature_selection import SelectFromModel
import warnings

warnings.filterwarnings('ignore')


class FinalExtraTreesOptimizer:
    def __init__(self):
        self.best_model = None
        self.feature_selector = None

    def targeted_feature_engineering(self, X, y=None):
        """é’ˆå¯¹æ€§ç‰¹å¾å·¥ç¨‹ï¼ŒåŸºäºç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("è¿›è¡Œé’ˆå¯¹æ€§ç‰¹å¾å·¥ç¨‹...")
        X_engineered = X.copy()

        # åŸºäºä¹‹å‰åˆ†æçš„é‡è¦ç‰¹å¾åˆ›å»ºäº¤äº’é¡¹
        important_features = ['feat_339', 'feat_348', 'feat_338', 'Length', 'feat_342',
                              'feat_347', 'feat_341', 'feat_340', 'feat_246', 'feat_22']

        # ä¸ºé‡è¦ç‰¹å¾åˆ›å»ºå¤šé¡¹å¼é¡¹
        for feat in important_features[:5]:  # å‰5ä¸ªæœ€é‡è¦ç‰¹å¾
            if feat in X.columns:
                X_engineered[f'{feat}_squared'] = X[feat] ** 2
                X_engineered[f'{feat}_log'] = np.log1p(np.abs(X[feat]))

        # åˆ›å»ºé‡è¦ç‰¹å¾ä¹‹é—´çš„äº¤äº’é¡¹
        for i in range(min(3, len(important_features))):
            for j in range(i + 1, min(6, len(important_features))):
                feat1, feat2 = important_features[i], important_features[j]
                if feat1 in X.columns and feat2 in X.columns:
                    X_engineered[f'{feat1}_x_{feat2}'] = X[feat1] * X[feat2]
                    X_engineered[f'{feat1}_div_{feat2}'] = X[feat1] / (X[feat2] + 1e-8)

        # é«˜çº§ç»Ÿè®¡ç‰¹å¾
        X_engineered['top5_mean'] = X[important_features[:5]].mean(axis=1)
        X_engineered['top5_std'] = X[important_features[:5]].std(axis=1)
        X_engineered['top10_max'] = X[important_features[:10]].max(axis=1)
        X_engineered['top10_min'] = X[important_features[:10]].min(axis=1)

        print(f"é’ˆå¯¹æ€§ç‰¹å¾å·¥ç¨‹åç»´åº¦: {X_engineered.shape}")
        return X_engineered

    def elite_feature_selection(self, X_train, y_train, X_test, n_features=200):
        """ç²¾è‹±ç‰¹å¾é€‰æ‹©ï¼Œä¸“æ³¨äºé‡è¦ç‰¹å¾"""
        print("è¿›è¡Œç²¾è‹±ç‰¹å¾é€‰æ‹©...")

        # ä½¿ç”¨æ›´å¼ºçš„åŸºæ¨¡å‹è¿›è¡Œç‰¹å¾é€‰æ‹©
        selector = ExtraTreesRegressor(
            n_estimators=500,
            max_depth=30,
            random_state=42,
            n_jobs=-1
        )

        feature_selector = SelectFromModel(
            selector,
            max_features=n_features,
            threshold=-np.inf
        )

        feature_selector.fit(X_train, y_train)
        selected_features = X_train.columns[feature_selector.get_support()]

        print(f"ç²¾è‹±ç‰¹å¾é€‰æ‹©å: {len(selected_features)} ä¸ªç‰¹å¾")

        # ç¡®ä¿åŒ…å«æœ€é‡è¦çš„ç‰¹å¾
        important_features = ['feat_339', 'feat_348', 'feat_338', 'Length', 'feat_342']
        for feat in important_features:
            if feat in X_train.columns and feat not in selected_features:
                selected_features = selected_features.append(pd.Index([feat]))

        return X_train[selected_features], X_test[selected_features]

    def create_champion_model(self, X_train, y_train):
        """åˆ›å»ºå† å†›æ¨¡å‹"""
        print("åˆ›å»ºå† å†›ExtraTreesæ¨¡å‹...")

        # åŸºäºä¹‹å‰æœ€ä½³ç»“æœçš„ä¼˜åŒ–é…ç½®
        champion_params = {
            'n_estimators': 1500,  # æ›´å¤šæ ‘
            'max_depth': 45,  # æ›´æ·±
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'max_features': 0.85,  # è°ƒæ•´ç‰¹å¾é‡‡æ ·
            'bootstrap': True,
            'oob_score': True,
            'random_state': 42,
            'n_jobs': -1,
            'min_impurity_decrease': 0.0001  # æ›´ç»†çš„åˆ†è£‚
        }

        model = ExtraTreesRegressor(**champion_params)
        model.fit(X_train, y_train)

        # è¯¦ç»†è¯„ä¼°
        cv_scores = cross_val_score(model, X_train, y_train,
                                    cv=5, scoring='r2', n_jobs=-1)

        print(f"å† å†›æ¨¡å‹æ€§èƒ½:")
        print(f"  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        print(f"  OOB Score: {model.oob_score_:.4f}")

        return model

    def advanced_ensemble_strategy(self, X_train, y_train, X_test, y_test, n_models=7):
        """é«˜çº§é›†æˆç­–ç•¥"""
        print("æ‰§è¡Œé«˜çº§é›†æˆç­–ç•¥...")

        # ä¸åŒçš„æ¨¡å‹é…ç½®å¢åŠ å¤šæ ·æ€§
        configs = [
            # æ·±åº¦ä¸“å®¶
            {'n_estimators': 1500, 'max_depth': 45, 'max_features': 0.85, 'random_state': 42},
            # å¹¿åº¦ä¸“å®¶
            {'n_estimators': 1200, 'max_depth': 35, 'max_features': 0.9, 'random_state': 43},
            # å¹³è¡¡ä¸“å®¶
            {'n_estimators': 1800, 'max_depth': 40, 'max_features': 0.8, 'random_state': 44},
            # ä¿å®ˆä¸“å®¶
            {'n_estimators': 1000, 'max_depth': 50, 'max_features': 0.75, 'random_state': 45},
            # æ¿€è¿›ä¸“å®¶
            {'n_estimators': 2000, 'max_depth': 30, 'max_features': 0.95, 'random_state': 46},
            # ç‰¹å¾ä¸“å®¶
            {'n_estimators': 1300, 'max_depth': 38, 'max_features': 0.7, 'random_state': 47},
            # æ•°æ®ä¸“å®¶
            {'n_estimators': 1600, 'max_depth': 42, 'max_features': 0.88, 'random_state': 48}
        ]

        models = []
        predictions = []
        performances = []

        print("è®­ç»ƒä¸“å®¶æ¨¡å‹:")
        for i, config in enumerate(configs[:n_models]):
            model = ExtraTreesRegressor(**config, bootstrap=True, oob_score=True, n_jobs=-1)
            model.fit(X_train, y_train)
            models.append(model)

            # è®­ç»ƒé›†æ€§èƒ½ï¼ˆç”¨äºæƒé‡è®¡ç®—ï¼‰
            train_pred = model.predict(X_train)
            train_r2 = r2_score(y_train, train_pred)
            performances.append(train_r2)

            # æµ‹è¯•é›†é¢„æµ‹
            test_pred = model.predict(X_test)
            predictions.append(test_pred)

            oob_score = getattr(model, 'oob_score_', 0)
            print(f"  ä¸“å®¶{i + 1}: CV RÂ²={train_r2:.4f}, OOB={oob_score:.4f}")

        # æ™ºèƒ½æƒé‡è®¡ç®—ï¼ˆåŸºäºè®­ç»ƒæ€§èƒ½ï¼‰
        weights = [max(perf, 0.3) for perf in performances]  # ç¡®ä¿æœ€å°æƒé‡
        weights = np.array(weights) ** 2  # å¹³æ–¹æ”¾å¤§å·®å¼‚
        weights = weights / np.sum(weights)

        # åŠ æƒé›†æˆ
        weighted_pred = np.zeros(len(X_test))
        for i, pred in enumerate(predictions):
            weighted_pred += weights[i] * pred

        # ä¸­ä½æ•°é›†æˆï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        median_pred = np.median(predictions, axis=0)

        # é€‰æ‹©æœ€ä½³é›†æˆæ–¹æ³•
        weighted_r2 = r2_score(y_test, weighted_pred)
        median_r2 = r2_score(y_test, median_pred)

        print(f"\né›†æˆæ–¹æ³•æ¯”è¾ƒ:")
        print(f"  åŠ æƒé›†æˆ RÂ²: {weighted_r2:.4f}")
        print(f"  ä¸­ä½æ•°é›†æˆ RÂ²: {median_r2:.4f}")

        if weighted_r2 >= median_r2:
            final_pred = weighted_pred
            print("  é€‰æ‹©åŠ æƒé›†æˆ")
        else:
            final_pred = median_pred
            print("  é€‰æ‹©ä¸­ä½æ•°é›†æˆ")

        print("ä¸“å®¶æƒé‡:")
        for i, weight in enumerate(weights):
            print(f"  ä¸“å®¶{i + 1}: {weight:.3f}")

        return final_pred, models, weighted_r2 if weighted_r2 >= median_r2 else median_r2

    def residual_correction(self, base_model, X_train, y_train, X_test, y_test):
        """æ®‹å·®ä¿®æ­£"""
        print("åº”ç”¨æ®‹å·®ä¿®æ­£...")

        # åŸºç¡€é¢„æµ‹
        base_pred_train = base_model.predict(X_train)
        residuals = y_train - base_pred_train

        # è®­ç»ƒæ®‹å·®é¢„æµ‹æ¨¡å‹
        residual_model = ExtraTreesRegressor(
            n_estimators=800,
            max_depth=25,
            max_features=0.7,
            random_state=42,
            n_jobs=-1
        )
        residual_model.fit(X_train, residuals)

        # é¢„æµ‹æ®‹å·®
        residual_pred = residual_model.predict(X_test)

        # ä¿®æ­£é¢„æµ‹ï¼ˆä½¿ç”¨è¡°å‡å› å­é¿å…è¿‡æ‹Ÿåˆï¼‰
        corrected_pred = base_model.predict(X_test) + residual_pred * 0.3

        corrected_r2 = r2_score(y_test, corrected_pred)
        base_r2 = r2_score(y_test, base_model.predict(X_test))

        print(f"æ®‹å·®ä¿®æ­£æ•ˆæœ:")
        print(f"  åŸºç¡€æ¨¡å‹ RÂ²: {base_r2:.4f}")
        print(f"  ä¿®æ­£å RÂ²: {corrected_r2:.4f}")
        print(f"  æ”¹è¿›: {corrected_r2 - base_r2:.4f}")

        if corrected_r2 > base_r2:
            return corrected_pred, corrected_r2
        else:
            return base_model.predict(X_test), base_r2

    def final_pipeline(self, X_train, y_train, X_test, y_test):
        """æœ€ç»ˆä¼˜åŒ–ç®¡é“"""
        print("å¼€å§‹æœ€ç»ˆä¼˜åŒ–ç®¡é“...")

        # 1. é’ˆå¯¹æ€§ç‰¹å¾å·¥ç¨‹
        print("\n1. é’ˆå¯¹æ€§ç‰¹å¾å·¥ç¨‹...")
        X_train_eng = self.targeted_feature_engineering(X_train)
        X_test_eng = self.targeted_feature_engineering(X_test)

        # 2. ç²¾è‹±ç‰¹å¾é€‰æ‹©
        print("\n2. ç²¾è‹±ç‰¹å¾é€‰æ‹©...")
        X_train_sel, X_test_sel = self.elite_feature_selection(X_train_eng, y_train, X_test_eng, 250)

        # 3. æ•°æ®å˜æ¢
        print("\n3. æ•°æ®å˜æ¢...")
        transformer = QuantileTransformer(output_distribution='normal', random_state=42)
        X_train_trans = pd.DataFrame(transformer.fit_transform(X_train_sel),
                                     columns=X_train_sel.columns, index=X_train_sel.index)
        X_test_trans = pd.DataFrame(transformer.transform(X_test_sel),
                                    columns=X_test_sel.columns, index=X_test_sel.index)

        # 4. åˆ›å»ºå† å†›æ¨¡å‹
        print("\n4. åˆ›å»ºå† å†›æ¨¡å‹...")
        champion_model = self.create_champion_model(X_train_trans, y_train)
        champion_pred = champion_model.predict(X_test_trans)
        champion_r2 = r2_score(y_test, champion_pred)

        # 5. é«˜çº§é›†æˆ
        print("\n5. é«˜çº§é›†æˆç­–ç•¥...")
        ensemble_pred, ensemble_models, ensemble_r2 = self.advanced_ensemble_strategy(
            X_train_trans, y_train, X_test_trans, y_test, n_models=7
        )

        # 6. æ®‹å·®ä¿®æ­£
        print("\n6. æ®‹å·®ä¿®æ­£...")
        # é€‰æ‹©åŸºç¡€æ¨¡å‹ï¼ˆå† å†›æ¨¡å‹æˆ–é›†æˆæ¨¡å‹ï¼‰
        if champion_r2 >= ensemble_r2:
            base_model = champion_model
            base_pred = champion_pred
            base_r2 = champion_r2
            print("  ä½¿ç”¨å† å†›æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹")
        else:
            # ä½¿ç”¨é›†æˆæ¨¡å‹ä¸­æœ€å¥½çš„ä¸€ä¸ªä½œä¸ºåŸºç¡€æ¨¡å‹
            best_model_idx = np.argmax([r2_score(y_test, model.predict(X_test_trans))
                                        for model in ensemble_models])
            base_model = ensemble_models[best_model_idx]
            base_pred = ensemble_pred
            base_r2 = ensemble_r2
            print("  ä½¿ç”¨æœ€ä½³ä¸“å®¶æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹")

        final_pred, final_r2 = self.residual_correction(base_model, X_train_trans, y_train, X_test_trans, y_test)

        # æœ€ç»ˆæ¯”è¾ƒ
        print(f"\n{'=' * 60}")
        print("æœ€ç»ˆæ–¹æ³•æ¯”è¾ƒ:")
        print(f"å† å†›æ¨¡å‹ RÂ²: {champion_r2:.4f}")
        print(f"é«˜çº§é›†æˆ RÂ²: {ensemble_r2:.4f}")
        print(f"æ®‹å·®ä¿®æ­£ RÂ²: {final_r2:.4f}")
        print(f"{'=' * 60}")

        # é€‰æ‹©æœ€ç»ˆæœ€ä½³æ–¹æ³•
        methods = {
            'champion': (champion_pred, champion_r2),
            'ensemble': (ensemble_pred, ensemble_r2),
            'final': (final_pred, final_r2)
        }

        best_method = max(methods.items(), key=lambda x: x[1][1])
        best_name, (best_pred, best_r2) = best_method

        print(f"\nğŸ¯ æœ€ç»ˆæœ€ä½³æ–¹æ³•: {best_name}, RÂ²: {best_r2:.4f}")

        self.best_model = base_model if best_name == 'final' else champion_model

        return best_pred, best_name, best_r2, methods


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    print("å¼€å§‹æœ€ç»ˆExtraTreesä¼˜åŒ–åˆ†æ...")

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # è®°å½•ä¹‹å‰çš„æœ€ä½³æ€§èƒ½
    previous_best_r2 = 0.3458
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")

    # åˆ›å»ºæœ€ç»ˆä¼˜åŒ–å™¨
    optimizer = FinalExtraTreesOptimizer()

    # æ‰§è¡Œæœ€ç»ˆç®¡é“
    final_pred, best_method, final_r2, all_methods = optimizer.final_pipeline(
        X_train, y_train, X_test, y_test
    )

    # æ€§èƒ½æ”¹è¿›åˆ†æ
    improvement = final_r2 - previous_best_r2
    improvement_percent = (improvement / previous_best_r2) * 100

    print(f"\n{'=' * 50}")
    print("æœ€ç»ˆæ€§èƒ½æ”¹è¿›æ€»ç»“")
    print(f"{'=' * 50}")
    print(f"èµ·å§‹RÂ²: 0.3279")
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")
    print(f"å½“å‰æœ€ä½³RÂ²: {final_r2:.4f}")
    print(f"æ€»ç»å¯¹æå‡: {final_r2 - 0.3279:.4f}")
    print(f"æœ¬æ¬¡æå‡: {improvement:.4f}")
    print(f"æœ¬æ¬¡ç›¸å¯¹æå‡: {improvement_percent:.2f}%")

    if improvement > 0:
        print("ğŸ‰ ä¼˜åŒ–æˆåŠŸï¼æ€§èƒ½ç»§ç»­æå‡ï¼")
    else:
        print("âš ï¸ æ€§èƒ½è¾¾åˆ°å¹³å°æœŸ")

    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'çœŸå®å€¼': y_test.values,
        'é¢„æµ‹å€¼': final_pred,
        'æ®‹å·®': y_test.values - final_pred
    })

    results_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\final_ensemble_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\næœ€ç»ˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    print(f"\næœ€ç»ˆä¼˜åŒ–åˆ†æå®Œæˆï¼æœ€ç»ˆRÂ²: {final_r2:.4f}")


if __name__ == "__main__":
    main()