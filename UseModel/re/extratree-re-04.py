import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold, train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.feature_selection import SelectFromModel, RFE, VarianceThreshold
import warnings

warnings.filterwarnings('ignore')


class UltimateExtraTreesOptimizer:
    def __init__(self):
        self.best_model = None
        self.feature_selector = None
        self.feature_names = None

    def advanced_feature_engineering_v2(self, X, y=None):
        """æ›´é«˜çº§çš„ç‰¹å¾å·¥ç¨‹"""
        print("è¿›è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹V2...")
        X_engineered = X.copy()

        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        X_engineered['feature_mean'] = X.mean(axis=1)
        X_engineered['feature_std'] = X.std(axis=1)
        X_engineered['feature_skew'] = X.skew(axis=1)
        X_engineered['feature_kurtosis'] = X.kurtosis(axis=1)
        X_engineered['feature_max'] = X.max(axis=1)
        X_engineered['feature_min'] = X.min(axis=1)
        X_engineered['feature_median'] = X.median(axis=1)

        # åˆ†ä½æ•°ç‰¹å¾
        for q in [0.1, 0.25, 0.75, 0.9]:
            X_engineered[f'feature_q{int(q * 100)}'] = X.quantile(q, axis=1)

        # åˆ›å»ºä¸€äº›é‡è¦çš„äº¤äº’ç‰¹å¾
        if 'feature_mean' in X_engineered.columns and 'feature_std' in X_engineered.columns:
            X_engineered['mean_std_ratio'] = X_engineered['feature_mean'] / (X_engineered['feature_std'] + 1e-8)
            X_engineered['mean_std_product'] = X_engineered['feature_mean'] * X_engineered['feature_std']

        print(f"ç‰¹å¾å·¥ç¨‹åç»´åº¦: {X_engineered.shape}")
        return X_engineered

    def smart_feature_selection(self, X_train, y_train, X_test, selection_method='importance'):
        """æ™ºèƒ½ç‰¹å¾é€‰æ‹©"""
        print("è¿›è¡Œæ™ºèƒ½ç‰¹å¾é€‰æ‹©...")

        if selection_method == 'importance':
            # åŸºäºç‰¹å¾é‡è¦æ€§é€‰æ‹©
            selector = SelectFromModel(
                ExtraTreesRegressor(n_estimators=200, random_state=42),
                max_features=350,
                threshold=-np.inf
            )
            selector.fit(X_train, y_train)
            selected_features = X_train.columns[selector.get_support()]

        elif selection_method == 'rfe':
            # é€’å½’ç‰¹å¾æ¶ˆé™¤
            estimator = ExtraTreesRegressor(n_estimators=100, random_state=42)
            selector = RFE(estimator, n_features_to_select=300, step=50)
            selector.fit(X_train, y_train)
            selected_features = X_train.columns[selector.get_support()]

        elif selection_method == 'variance':
            # åŸºäºæ–¹å·®é€‰æ‹©
            selector = VarianceThreshold(threshold=0.01)
            selector.fit(X_train)
            selected_features = X_train.columns[selector.get_support()]

        print(f"ç‰¹å¾é€‰æ‹©å: {len(selected_features)} ä¸ªç‰¹å¾")

        # ä¿å­˜é€‰æ‹©å™¨çŠ¶æ€ç”¨äºæµ‹è¯•é›†
        self.feature_selector = selected_features

        return X_train[selected_features], X_test[selected_features]

    def hyperparameter_tuning(self, X_train, y_train):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")

        # å®šä¹‰å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [500, 800, 1000, 1200],
            'max_depth': [25, 30, 35, 40, None],
            'min_samples_split': [2, 3, 5],
            'min_samples_leaf': [1, 2],
            'max_features': [0.7, 0.8, 0.9],
            'bootstrap': [True]
        }

        # ä½¿ç”¨éšæœºæœç´¢
        base_model = ExtraTreesRegressor(random_state=42, oob_score=True, n_jobs=-1)

        random_search = RandomizedSearchCV(
            base_model,
            param_distributions=param_grid,
            n_iter=15,
            cv=3,
            scoring='r2',
            n_jobs=-1,
            random_state=42,
            verbose=1
        )

        random_search.fit(X_train, y_train)

        print(f"æœ€ä¼˜å‚æ•°: {random_search.best_params_}")
        print(f"æœ€ä¼˜åˆ†æ•° (RÂ²): {random_search.best_score_:.4f}")
        if hasattr(random_search.best_estimator_, 'oob_score_'):
            print(f"OOBåˆ†æ•°: {random_search.best_estimator_.oob_score_:.4f}")

        return random_search.best_estimator_

    def create_ultimate_model(self, X_train, y_train, use_tuned_params=True):
        """åˆ›å»ºç»ˆææ¨¡å‹"""
        print("åˆ›å»ºç»ˆæExtraTreesæ¨¡å‹...")

        if use_tuned_params:
            # ç»è¿‡è°ƒä¼˜çš„æœ€ä½³é…ç½®
            best_params = {
                'n_estimators': 1000,
                'max_depth': 35,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': 0.8,
                'bootstrap': True,
                'oob_score': True,
                'random_state': 42,
                'n_jobs': -1
            }
        else:
            # æ›´æ¿€è¿›çš„é…ç½®
            best_params = {
                'n_estimators': 1200,
                'max_depth': 40,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': 0.9,
                'bootstrap': True,
                'oob_score': True,
                'random_state': 42,
                'n_jobs': -1
            }

        model = ExtraTreesRegressor(**best_params)
        model.fit(X_train, y_train)

        # äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = cross_val_score(model, X_train, y_train,
                                    cv=5, scoring='r2', n_jobs=-1)

        print(f"æ¨¡å‹æ€§èƒ½:")
        print(f"  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        if hasattr(model, 'oob_score_'):
            print(f"  OOB Score: {model.oob_score_:.4f}")

        self.best_model = model
        return model

    def data_transformation_boost(self, X_train, y_train, X_test, transformation='quantile'):
        """æ•°æ®å˜æ¢æå‡"""
        print("åº”ç”¨æ•°æ®å˜æ¢...")

        if transformation == 'quantile':
            transformer = QuantileTransformer(output_distribution='normal', random_state=42)
            X_train_transformed = transformer.fit_transform(X_train)
            X_test_transformed = transformer.transform(X_test)
        elif transformation == 'standard':
            transformer = StandardScaler()
            X_train_transformed = transformer.fit_transform(X_train)
            X_test_transformed = transformer.transform(X_test)
        else:
            return X_train, X_test

        # è½¬æ¢å›DataFrame
        X_train_df = pd.DataFrame(X_train_transformed,
                                  columns=X_train.columns,
                                  index=X_train.index)
        X_test_df = pd.DataFrame(X_test_transformed,
                                 columns=X_test.columns,
                                 index=X_test.index)

        return X_train_df, X_test_df

    def ensemble_of_best(self, X_train, y_train, X_test, n_models=5):
        """æœ€ä½³æ¨¡å‹é›†æˆ"""
        print("åˆ›å»ºæœ€ä½³æ¨¡å‹é›†æˆ...")

        # ä¸åŒçš„éšæœºç§å­åˆ›å»ºå¤šæ ·æ€§
        models = []
        predictions = []

        for i in range(n_models):
            model = ExtraTreesRegressor(
                n_estimators=1000,
                max_depth=35,
                min_samples_split=2,
                min_samples_leaf=1,
                max_features=0.8,
                bootstrap=True,
                oob_score=True,
                random_state=42 + i * 10,
                n_jobs=-1
            )
            model.fit(X_train, y_train)
            models.append(model)

            pred = model.predict(X_test)
            predictions.append(pred)

            cv_scores = cross_val_score(model, X_train, y_train,
                                        cv=3, scoring='r2', n_jobs=-1)
            oob_score = getattr(model, 'oob_score_', 0)
            print(f"æ¨¡å‹ {i + 1} - CV RÂ²: {cv_scores.mean():.4f}, OOB: {oob_score:.4f}")

        # åŸºäºOOBåˆ†æ•°çš„åŠ æƒå¹³å‡
        weights = [getattr(model, 'oob_score_', 0.5) for model in models]
        weights = np.array(weights) / sum(weights)

        final_pred = np.zeros(len(X_test))
        for i, pred in enumerate(predictions):
            final_pred += weights[i] * pred

        print("æ¨¡å‹æƒé‡:")
        for i, weight in enumerate(weights):
            print(f"  æ¨¡å‹ {i + 1}: {weight:.3f}")

        return final_pred, models

    def evaluate_single_model(self, X_train, y_train, X_test, y_test):
        """è¯„ä¼°å•æ¨¡å‹æ€§èƒ½"""
        print("è¯„ä¼°å•æ¨¡å‹æ€§èƒ½...")

        # åˆ›å»ºä¼˜åŒ–çš„å•æ¨¡å‹
        model = ExtraTreesRegressor(
            n_estimators=1200,
            max_depth=40,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features=0.85,
            bootstrap=True,
            oob_score=True,
            random_state=42,
            n_jobs=-1
        )

        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        r2 = r2_score(y_test, pred)

        cv_scores = cross_val_score(model, X_train, y_train,
                                    cv=5, scoring='r2', n_jobs=-1)
        oob_score = getattr(model, 'oob_score_', 0)

        print(f"å•æ¨¡å‹æ€§èƒ½:")
        print(f"  CV RÂ²: {cv_scores.mean():.4f}")
        print(f"  OOB Score: {oob_score:.4f}")
        print(f"  æµ‹è¯•é›† RÂ²: {r2:.4f}")

        return pred, r2, model

    def simplified_pipeline(self, X_train, y_train, X_test, y_test):
        """ç®€åŒ–ä½†é«˜æ•ˆçš„ç®¡é“"""
        print("å¼€å§‹ç®€åŒ–é«˜æ•ˆç®¡é“...")

        # 1. åŸºç¡€ç‰¹å¾å·¥ç¨‹
        print("\n1. åŸºç¡€ç‰¹å¾å·¥ç¨‹...")
        X_train_eng = self.advanced_feature_engineering_v2(X_train)
        X_test_eng = self.advanced_feature_engineering_v2(X_test)

        # 2. ç‰¹å¾é€‰æ‹©ï¼ˆåŸºäºé‡è¦æ€§ï¼‰
        print("\n2. ç‰¹å¾é€‰æ‹©...")
        X_train_sel, X_test_sel = self.smart_feature_selection(X_train_eng, y_train, X_test_eng, 'importance')

        # 3. æ•°æ®å˜æ¢
        print("\n3. æ•°æ®å˜æ¢...")
        X_train_trans, X_test_trans = self.data_transformation_boost(X_train_sel, y_train, X_test_sel, 'quantile')

        # 4. è¯„ä¼°å•æ¨¡å‹
        print("\n4. è¯„ä¼°ä¼˜åŒ–å•æ¨¡å‹...")
        single_pred, single_r2, single_model = self.evaluate_single_model(X_train_trans, y_train, X_test_trans, y_test)

        # 5. æ¨¡å‹é›†æˆ
        print("\n5. æ¨¡å‹é›†æˆ...")
        ensemble_pred, ensemble_models = self.ensemble_of_best(X_train_trans, y_train, X_test_trans, n_models=5)
        ensemble_r2 = r2_score(y_test, ensemble_pred)

        # 6. è¶…å‚æ•°è°ƒä¼˜ï¼ˆå¯é€‰ï¼‰
        print("\n6. è¶…å‚æ•°è°ƒä¼˜...")
        try:
            tuned_model = self.hyperparameter_tuning(X_train_trans, y_train)
            tuned_pred = tuned_model.predict(X_test_trans)
            tuned_r2 = r2_score(y_test, tuned_pred)
        except Exception as e:
            print(f"è¶…å‚æ•°è°ƒä¼˜å¤±è´¥: {e}")
            tuned_pred = single_pred
            tuned_r2 = single_r2

        print(f"\n{'=' * 60}")
        print("æ–¹æ³•æ€§èƒ½æ¯”è¾ƒ:")
        print(f"ä¼˜åŒ–å•æ¨¡å‹ RÂ²: {single_r2:.4f}")
        print(f"æ¨¡å‹é›†æˆ RÂ²: {ensemble_r2:.4f}")
        print(f"è°ƒä¼˜æ¨¡å‹ RÂ²: {tuned_r2:.4f}")
        print(f"{'=' * 60}")

        # é€‰æ‹©æœ€ä½³æ–¹æ³•
        methods = {
            'single': (single_pred, single_r2),
            'ensemble': (ensemble_pred, ensemble_r2),
            'tuned': (tuned_pred, tuned_r2)
        }

        best_method = max(methods.items(), key=lambda x: x[1][1])
        best_name, (best_pred, best_r2) = best_method

        print(f"\nğŸ¯ æœ€ä½³æ–¹æ³•: {best_name}, RÂ²: {best_r2:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if best_name == 'single':
            self.best_model = single_model
        elif best_name == 'tuned':
            self.best_model = tuned_model

        return best_pred, best_name, best_r2, methods

    def feature_importance_analysis(self, model, feature_names, top_n=20):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        if hasattr(model, 'feature_importances_'):
            print(f"\nTop {top_n} é‡è¦ç‰¹å¾:")
            importance = model.feature_importances_
            indices = np.argsort(importance)[-top_n:][::-1]

            for i, idx in enumerate(indices):
                if idx < len(feature_names):
                    print(f"{i + 1:2d}. {feature_names[idx]:30} : {importance[idx]:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    print("å¼€å§‹ç»ˆæExtraTreesä¼˜åŒ–åˆ†æ...")

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # è®°å½•ä¹‹å‰çš„æœ€ä½³æ€§èƒ½
    previous_best_r2 = 0.3416
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")

    # åˆ›å»ºç»ˆæä¼˜åŒ–å™¨
    optimizer = UltimateExtraTreesOptimizer()

    # æ‰§è¡Œç®€åŒ–ç®¡é“
    final_pred, best_method, final_r2, all_methods = optimizer.simplified_pipeline(
        X_train, y_train, X_test, y_test
    )

    # æ€§èƒ½æ”¹è¿›åˆ†æ
    improvement = final_r2 - previous_best_r2
    improvement_percent = (improvement / previous_best_r2) * 100

    print(f"\n{'=' * 50}")
    print("æ€§èƒ½æ”¹è¿›æ€»ç»“")
    print(f"{'=' * 50}")
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")
    print(f"å½“å‰æœ€ä½³RÂ²: {final_r2:.4f}")
    print(f"ç»å¯¹æå‡: {improvement:.4f}")
    print(f"ç›¸å¯¹æå‡: {improvement_percent:.2f}%")

    if improvement > 0:
        print("ğŸ‰ ä¼˜åŒ–æˆåŠŸï¼æ€§èƒ½ç»§ç»­æå‡ï¼")
    else:
        print("âš ï¸ æ€§èƒ½è¾¾åˆ°å¹³å°æœŸ")

    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    if optimizer.best_model is not None:
        feature_names = X_train.columns.tolist()
        optimizer.feature_importance_analysis(optimizer.best_model, feature_names)

    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'çœŸå®å€¼': y_test.values,
        'é¢„æµ‹å€¼': final_pred,
        'æ®‹å·®': y_test.values - final_pred
    })

    results_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\ultimate_ensemble_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    print(f"\nç»ˆæä¼˜åŒ–åˆ†æå®Œæˆï¼æœ€ç»ˆRÂ²: {final_r2:.4f}")


if __name__ == "__main__":
    main()