import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
import warnings

warnings.filterwarnings('ignore')


class AdvancedOptimizationStrategies:
    def __init__(self):
        self.strategies = {}

    def analyze_remaining_potential(self, X_train, y_train, X_test, y_test, current_r2=0.3539):
        """åˆ†æå‰©ä½™ä¼˜åŒ–æ½œåŠ›"""
        print("åˆ†æè¿›ä¸€æ­¥ä¼˜åŒ–æ½œåŠ›...")

        # 1. æ£€æŸ¥ç‰¹å¾ä¸ç›®æ ‡çš„å…³ç³»éçº¿æ€§
        print("\n1. ç‰¹å¾-ç›®æ ‡å…³ç³»åˆ†æ")
        correlation_with_target = X_train.corrwith(y_train).abs()
        high_corr_features = correlation_with_target[correlation_with_target > 0.1]
        print(f"ä¸ç›®æ ‡ç›¸å…³æ€§>0.1çš„ç‰¹å¾æ•°é‡: {len(high_corr_features)}")

        # 2. æ£€æŸ¥æ¨¡å‹å¤æ‚åº¦æ˜¯å¦è¶³å¤Ÿ
        print("\n2. æ¨¡å‹å¤æ‚åº¦åˆ†æ")
        test_models = [
            ('ä¿å®ˆ', {'n_estimators': 500, 'max_depth': 20}),
            ('å¹³è¡¡', {'n_estimators': 1000, 'max_depth': 30}),
            ('æ¿€è¿›', {'n_estimators': 2000, 'max_depth': 50}),
            ('è¶…æ¿€è¿›', {'n_estimators': 3000, 'max_depth': None})
        ]

        for name, params in test_models:
            model = ExtraTreesRegressor(**params, random_state=42, n_jobs=-1)
            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='r2')
            print(f"  {name}æ¨¡å‹ - CV RÂ²: {cv_scores.mean():.4f}")

        # 3. æ£€æŸ¥æ•°æ®è´¨é‡é—®é¢˜
        print("\n3. æ•°æ®è´¨é‡åˆ†æ")
        print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}")
        print(f"  ç‰¹å¾æ•°é‡: {X_train.shape[1]}")
        print(f"  ç›®æ ‡å˜é‡èŒƒå›´: [{y_train.min():.3f}, {y_train.max():.3f}]")
        print(f"  ç›®æ ‡å˜é‡æ ‡å‡†å·®: {y_train.std():.3f}")

        return len(high_corr_features)


class NextLevelOptimizer:
    def __init__(self):
        self.best_r2 = 0.3539

    def deep_feature_engineering(self, X, y=None):
        """æ·±åº¦ç‰¹å¾å·¥ç¨‹"""
        print("è¿›è¡Œæ·±åº¦ç‰¹å¾å·¥ç¨‹...")
        X_deep = X.copy()

        # åŸºäºé¢†åŸŸçŸ¥è¯†çš„ç‰¹å¾æ„é€ 
        # 1. èšç±»ç‰¹å¾
        from sklearn.cluster import KMeans
        try:
            kmeans = KMeans(n_clusters=5, random_state=42)
            cluster_labels = kmeans.fit_predict(X)
            X_deep['cluster'] = cluster_labels
        except:
            pass

        # 2. ä¸»æˆåˆ†ç‰¹å¾
        from sklearn.decomposition import PCA
        try:
            pca = PCA(n_components=10, random_state=42)
            pca_features = pca.fit_transform(X)
            for i in range(5):
                X_deep[f'pca_{i + 1}'] = pca_features[:, i]
        except:
            pass

        # 3. å¤šé¡¹å¼ç‰¹å¾ï¼ˆé€‰æ‹©æ€§ï¼‰
        important_features = ['feat_339', 'feat_348', 'feat_338', 'Length', 'feat_342']
        for feat in important_features:
            if feat in X.columns:
                # é«˜é˜¶å¤šé¡¹å¼
                X_deep[f'{feat}_cubed'] = X[feat] ** 3
                X_deep[f'{feat}_exp'] = np.exp(0.1 * np.abs(X[feat]))

        print(f"æ·±åº¦ç‰¹å¾å·¥ç¨‹åç»´åº¦: {X_deep.shape}")
        return X_deep

    def advanced_ensemble_architectures(self, X_train, y_train, X_test, y_test):
        """é«˜çº§é›†æˆæ¶æ„"""
        print("å°è¯•é«˜çº§é›†æˆæ¶æ„...")

        # 1. åˆ†å±‚é›†æˆ
        from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

        # ç¬¬ä¸€å±‚ï¼šå¤šç§ç®—æ³•
        layer1_models = {
            'et1': ExtraTreesRegressor(n_estimators=1000, max_depth=35, random_state=1, n_jobs=-1),
            'et2': ExtraTreesRegressor(n_estimators=800, max_depth=40, random_state=2, n_jobs=-1),
            'gb': GradientBoostingRegressor(n_estimators=500, max_depth=6, random_state=42),
            'rf': RandomForestRegressor(n_estimators=800, max_depth=30, random_state=42, n_jobs=-1)
        }

        # è®­ç»ƒç¬¬ä¸€å±‚
        layer1_predictions_train = []
        layer1_predictions_test = []

        for name, model in layer1_models.items():
            print(f"  è®­ç»ƒç¬¬ä¸€å±‚æ¨¡å‹: {name}")
            model.fit(X_train, y_train)
            layer1_predictions_train.append(model.predict(X_train))
            layer1_predictions_test.append(model.predict(X_test))

        # åˆ›å»ºç¬¬äºŒå±‚ç‰¹å¾
        X_train_layer2 = np.column_stack(layer1_predictions_train)
        X_test_layer2 = np.column_stack(layer1_predictions_test)

        # ç¬¬äºŒå±‚ï¼šå…ƒå­¦ä¹ å™¨
        meta_learners = {
            'linear': ExtraTreesRegressor(n_estimators=500, max_depth=20, random_state=42),
            'nonlinear': GradientBoostingRegressor(n_estimators=300, random_state=42)
        }

        best_meta_r2 = -1
        best_meta_pred = None

        for name, meta_learner in meta_learners.items():
            meta_learner.fit(X_train_layer2, y_train)
            meta_pred = meta_learner.predict(X_test_layer2)
            meta_r2 = r2_score(y_test, meta_pred)

            print(f"  å…ƒå­¦ä¹ å™¨ {name}: RÂ² = {meta_r2:.4f}")

            if meta_r2 > best_meta_r2:
                best_meta_r2 = meta_r2
                best_meta_pred = meta_pred

        return best_meta_pred, best_meta_r2

    def target_transformation(self, X_train, y_train, X_test, y_test):
        """ç›®æ ‡å˜é‡å˜æ¢"""
        print("å°è¯•ç›®æ ‡å˜é‡å˜æ¢...")

        # 1. å¯¹æ•°å˜æ¢
        y_train_log = np.log1p(y_train - y_train.min() + 1e-8)

        model_log = ExtraTreesRegressor(n_estimators=1000, max_depth=35, random_state=42, n_jobs=-1)
        model_log.fit(X_train, y_train_log)
        pred_log = model_log.predict(X_test)
        pred_original = np.expm1(pred_log) + y_train.min() - 1e-8
        r2_log = r2_score(y_test, pred_original)

        # 2. Box-Coxå˜æ¢
        from scipy import stats
        try:
            y_train_boxcox, lambda_val = stats.boxcox(y_train - y_train.min() + 1)
            model_boxcox = ExtraTreesRegressor(n_estimators=1000, max_depth=35, random_state=42, n_jobs=-1)
            model_boxcox.fit(X_train, y_train_boxcox)
            pred_boxcox = model_boxcox.predict(X_test)
            pred_original_bc = stats.inv_boxcox(pred_boxcox, lambda_val) + y_train.min() - 1
            r2_boxcox = r2_score(y_test, pred_original_bc)
        except:
            r2_boxcox = -1

        print(f"  å¯¹æ•°å˜æ¢ RÂ²: {r2_log:.4f}")
        print(f"  Box-Coxå˜æ¢ RÂ²: {r2_boxcox:.4f}")

        if r2_log > self.best_r2 or r2_boxcox > self.best_r2:
            best_r2 = max(r2_log, r2_boxcox, self.best_r2)
            if r2_log == best_r2:
                return pred_original, r2_log, 'log'
            else:
                return pred_original_bc, r2_boxcox, 'boxcox'

        return None, self.best_r2, 'none'

    def outlier_robust_training(self, X_train, y_train, X_test, y_test):
        """å¼‚å¸¸å€¼é²æ£’è®­ç»ƒ"""
        print("å°è¯•å¼‚å¸¸å€¼é²æ£’è®­ç»ƒ...")

        # 1. è¯†åˆ«å¼‚å¸¸å€¼
        from sklearn.ensemble import IsolationForest
        iso_forest = IsolationForest(contamination=0.05, random_state=42)
        outlier_labels = iso_forest.fit_predict(X_train)

        # 2. ä½¿ç”¨éå¼‚å¸¸å€¼è®­ç»ƒ
        inlier_mask = outlier_labels == 1
        X_train_clean = X_train[inlier_mask]
        y_train_clean = y_train[inlier_mask]

        print(f"  ç§»é™¤å¼‚å¸¸å€¼åè®­ç»ƒé›†å¤§å°: {len(X_train_clean)}/{len(X_train)}")

        model_clean = ExtraTreesRegressor(n_estimators=1000, max_depth=35, random_state=42, n_jobs=-1)
        model_clean.fit(X_train_clean, y_train_clean)
        pred_clean = model_clean.predict(X_test)
        r2_clean = r2_score(y_test, pred_clean)

        print(f"  å¼‚å¸¸å€¼é²æ£’è®­ç»ƒ RÂ²: {r2_clean:.4f}")

        return pred_clean, r2_clean

    def neural_network_hybrid(self, X_train, y_train, X_test, y_test):
        """ç¥ç»ç½‘ç»œæ··åˆæ–¹æ³•"""
        print("å°è¯•ç¥ç»ç½‘ç»œæ··åˆæ–¹æ³•...")

        try:
            from sklearn.neural_network import MLPRegressor
            from sklearn.preprocessing import StandardScaler

            # æ ‡å‡†åŒ–æ•°æ®
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # ç¥ç»ç½‘ç»œæ¨¡å‹
            nn_model = MLPRegressor(
                hidden_layer_sizes=(100, 50, 25),
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate='adaptive',
                max_iter=1000,
                random_state=42,
                early_stopping=True
            )

            nn_model.fit(X_train_scaled, y_train)
            nn_pred = nn_model.predict(X_test_scaled)
            nn_r2 = r2_score(y_test, nn_pred)

            print(f"  ç¥ç»ç½‘ç»œ RÂ²: {nn_r2:.4f}")

            # ä¸ExtraTreesæ··åˆ
            et_model = ExtraTreesRegressor(n_estimators=1000, max_depth=35, random_state=42, n_jobs=-1)
            et_model.fit(X_train, y_train)
            et_pred = et_model.predict(X_test)
            et_r2 = r2_score(y_test, et_pred)

            # ç®€å•æ··åˆ
            hybrid_pred = 0.7 * et_pred + 0.3 * nn_pred
            hybrid_r2 = r2_score(y_test, hybrid_pred)

            print(f"  æ··åˆæ¨¡å‹ RÂ²: {hybrid_r2:.4f}")

            return hybrid_pred, hybrid_r2

        except Exception as e:
            print(f"  ç¥ç»ç½‘ç»œæ–¹æ³•å¤±è´¥: {e}")
            return None, self.best_r2


def explore_optimization_potential():
    """æ¢ç´¢ä¼˜åŒ–æ½œåŠ›"""
    print("=" * 70)
    print("æ¢ç´¢ExtraTreesæ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–æ½œåŠ›")
    print("=" * 70)

    # åŠ è½½æ•°æ®
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    # åˆ†ææ½œåŠ›
    analyzer = AdvancedOptimizationStrategies()
    high_corr_count = analyzer.analyze_remaining_potential(X_train, y_train, X_test, y_test)

    print(f"\nğŸ¯ ä¼˜åŒ–æ½œåŠ›è¯„ä¼°:")
    if high_corr_count < 10:
        print("  âœ… ç‰¹å¾ä¸ç›®æ ‡ç›¸å…³æ€§è¾ƒä½ï¼Œç‰¹å¾å·¥ç¨‹æœ‰è¾ƒå¤§æ½œåŠ›")
    else:
        print("  âœ… å­˜åœ¨å¤šä¸ªç›¸å…³ç‰¹å¾ï¼Œæ¨¡å‹å¤æ‚åº¦å¯èƒ½æˆä¸ºç“¶é¢ˆ")

    print("  âœ… å¯ä»¥å°è¯•æ›´é«˜çº§çš„é›†æˆæ¶æ„")
    print("  âœ… ç›®æ ‡å˜é‡å˜æ¢å¯èƒ½æ”¹å–„æ¨¡å‹æ€§èƒ½")
    print("  âœ… å¼‚å¸¸å€¼å¤„ç†å¯èƒ½æå‡æ¨¡å‹é²æ£’æ€§")
    print("  âœ… ç¥ç»ç½‘ç»œæ··åˆæ–¹æ³•å€¼å¾—å°è¯•")


if __name__ == "__main__":
    explore_optimization_potential()