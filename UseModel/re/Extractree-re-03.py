import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel, VarianceThreshold
import warnings

warnings.filterwarnings('ignore')


class RobustExtraTreesOptimizer:
    def __init__(self):
        self.models = {}
        self.feature_selector = None
        self.feature_names = None

    def safe_feature_engineering(self, X, y=None):
        """å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹ï¼Œç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾ä¸€è‡´"""
        print("è¿›è¡Œå®‰å…¨çš„ç‰¹å¾å·¥ç¨‹...")
        X_engineered = X.copy()

        # åªåˆ›å»ºç¡®å®šçš„ç»Ÿè®¡ç‰¹å¾ï¼Œä¸ä¾èµ–å…·ä½“ç‰¹å¾åç§°
        X_engineered['feature_mean'] = X.mean(axis=1)
        X_engineered['feature_std'] = X.std(axis=1)
        X_engineered['feature_max'] = X.max(axis=1)
        X_engineered['feature_min'] = X.min(axis=1)
        X_engineered['feature_median'] = X.median(axis=1)

        # åˆ›å»ºåˆ†ä½æ•°ç‰¹å¾
        X_engineered['feature_q25'] = X.quantile(0.25, axis=1)
        X_engineered['feature_q75'] = X.quantile(0.75, axis=1)

        print(f"ç‰¹å¾å·¥ç¨‹åç»´åº¦: {X_engineered.shape}")
        return X_engineered

    def consistent_feature_selection(self, X_train, y_train, X_test, n_features=300):
        """ä¸€è‡´çš„ç‰¹å¾é€‰æ‹©ï¼Œç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾ç›¸åŒ"""
        print(f"è¿›è¡Œä¸€è‡´çš„ç‰¹å¾é€‰æ‹©ï¼Œç›®æ ‡ç‰¹å¾æ•°: {n_features}")

        # ä½¿ç”¨ç›¸åŒçš„é€‰æ‹©å™¨å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        if self.feature_selector is None:
            # åˆå§‹åŒ–é€‰æ‹©å™¨
            base_estimator = ExtraTreesRegressor(n_estimators=100, random_state=42)
            self.feature_selector = SelectFromModel(
                base_estimator,
                max_features=n_features,
                threshold=-np.inf
            )
            self.feature_selector.fit(X_train, y_train)
            self.feature_names = X_train.columns[self.feature_selector.get_support()]

        # ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©å™¨
        X_train_selected = self.feature_selector.transform(X_train)
        X_test_selected = self.feature_selector.transform(X_test)

        # è½¬æ¢ä¸ºDataFrameä¿æŒç‰¹å¾åç§°
        X_train_df = pd.DataFrame(X_train_selected,
                                  columns=self.feature_names,
                                  index=X_train.index)
        X_test_df = pd.DataFrame(X_test_selected,
                                 columns=self.feature_names,
                                 index=X_test.index)

        print(f"ç‰¹å¾é€‰æ‹©å: {X_train_df.shape[1]} ä¸ªç‰¹å¾")
        return X_train_df, X_test_df

    def create_robust_extra_trees(self, X_train, y_train):
        """åˆ›å»ºé²æ£’çš„ExtraTreesæ¨¡å‹"""
        print("åˆ›å»ºé²æ£’ExtraTreesæ¨¡å‹...")

        # æ›´ç¨³å®šçš„å‚æ•°é…ç½®
        robust_configs = [
            {
                'name': 'robust_deep',
                'params': {
                    'n_estimators': 800,
                    'max_depth': 40,
                    'min_samples_split': 3,
                    'min_samples_leaf': 2,
                    'max_features': 0.8,
                    'bootstrap': True,
                    'random_state': 42,
                    'oob_score': True
                }
            },
            {
                'name': 'robust_balanced',
                'params': {
                    'n_estimators': 600,
                    'max_depth': 30,
                    'min_samples_split': 5,
                    'min_samples_leaf': 3,
                    'max_features': 0.7,
                    'bootstrap': True,
                    'random_state': 43,
                    'oob_score': True
                }
            },
            {
                'name': 'robust_conservative',
                'params': {
                    'n_estimators': 1000,
                    'max_depth': 25,
                    'min_samples_split': 8,
                    'min_samples_leaf': 4,
                    'max_features': 0.6,
                    'bootstrap': True,
                    'random_state': 44,
                    'oob_score': True
                }
            },
            {
                'name': 'robust_aggressive',
                'params': {
                    'n_estimators': 500,
                    'max_depth': 50,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 0.9,
                    'bootstrap': True,
                    'random_state': 45,
                    'oob_score': True
                }
            }
        ]

        models = {}
        for config in robust_configs:
            print(f"è®­ç»ƒ {config['name']}...")
            model = ExtraTreesRegressor(**config['params'], n_jobs=-1)
            model.fit(X_train, y_train)
            models[config['name']] = model

            # è¯¦ç»†è¯„ä¼°
            cv_scores = cross_val_score(model, X_train, y_train,
                                        cv=5, scoring='r2', n_jobs=-1)
            mean_r2 = cv_scores.mean()
            std_r2 = cv_scores.std()

            oob_score = getattr(model, 'oob_score_', 0)

            print(f"  {config['name']:20} | CV RÂ²: {mean_r2:.4f} | OOB: {oob_score:.4f}")

        return models

    def safe_weighted_ensemble(self, models, X_train, y_train, X_test):
        """å®‰å…¨çš„åŠ æƒé›†æˆ"""
        print("æ‰§è¡Œå®‰å…¨çš„åŠ æƒé›†æˆ...")

        weights = {}
        predictions = {}

        for name, model in models.items():
            # ä½¿ç”¨OOBåˆ†æ•°æˆ–äº¤å‰éªŒè¯è®¡ç®—æƒé‡
            if hasattr(model, 'oob_score_') and model.oob_score_ > 0:
                weight = model.oob_score_
            else:
                cv_scores = cross_val_score(model, X_train, y_train,
                                            cv=3, scoring='r2', n_jobs=-1)
                weight = max(cv_scores.mean(), 0.1)

            weights[name] = weight
            predictions[name] = model.predict(X_test)

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        weights = {k: v / total_weight for k, v in weights.items()}

        # åŠ æƒé¢„æµ‹
        final_pred = np.zeros(len(X_test))
        for name, weight in weights.items():
            final_pred += weight * predictions[name]

        print("æ¨¡å‹æƒé‡åˆ†é…:")
        for name, weight in weights.items():
            print(f"  {name:20} | æƒé‡: {weight:.3f}")

        return final_pred, weights

    def kfold_blending(self, models, X_train, y_train, X_test, n_folds=5):
        """KæŠ˜æ··åˆé›†æˆ"""
        print("æ‰§è¡ŒKæŠ˜æ··åˆé›†æˆ...")

        from sklearn.linear_model import Ridge

        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºOOFé¢„æµ‹
        oof_predictions = np.zeros((len(X_train), len(models)))
        test_predictions = np.zeros((len(X_test), len(models)))

        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

        model_names = list(models.keys())

        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):
            print(f"  æŠ˜å  {fold + 1}/{n_folds}...")
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

            # åœ¨è®­ç»ƒæŠ˜ä¸Šè®­ç»ƒæ‰€æœ‰æ¨¡å‹
            fold_models = {}
            for name in model_names:
                model_params = models[name].get_params()
                model = ExtraTreesRegressor(**model_params, n_jobs=-1)
                model.fit(X_tr, y_tr)
                fold_models[name] = model

                # OOFé¢„æµ‹
                oof_predictions[val_idx, model_names.index(name)] = model.predict(X_val)
                # æµ‹è¯•é›†é¢„æµ‹ï¼ˆå¹³å‡ï¼‰
                test_predictions[:, model_names.index(name)] += model.predict(X_test) / n_folds

        # è®­ç»ƒå…ƒå­¦ä¹ å™¨
        meta_learner = Ridge(alpha=1.0)
        meta_learner.fit(oof_predictions, y_train)

        # æœ€ç»ˆé¢„æµ‹
        final_pred = meta_learner.predict(test_predictions)

        print(f"æ··åˆé›†æˆå®Œæˆï¼Œå…ƒæ¨¡å‹ç³»æ•°: {meta_learner.coef_}")
        return final_pred

    def smart_median_ensemble(self, models, X_test, use_weights=True):
        """æ™ºèƒ½ä¸­ä½æ•°é›†æˆ"""
        print("æ‰§è¡Œæ™ºèƒ½ä¸­ä½æ•°é›†æˆ...")

        all_predictions = []
        weights = []

        for name, model in models.items():
            pred = model.predict(X_test)
            all_predictions.append(pred)

            if use_weights and hasattr(model, 'oob_score_'):
                weights.append(model.oob_score_)
            else:
                weights.append(1.0)  # ç­‰æƒé‡

        all_predictions = np.array(all_predictions)

        if use_weights:
            # åŠ æƒä¸­ä½æ•°
            sorted_indices = np.argsort(all_predictions, axis=0)
            cumulative_weights = np.cumsum(np.array(weights)[sorted_indices], axis=0)
            median_idx = np.argmax(cumulative_weights >= np.sum(weights) / 2.0, axis=0)
            final_pred = all_predictions[sorted_indices[median_idx, np.arange(len(X_test))],
            np.arange(len(X_test))]
        else:
            # ç®€å•ä¸­ä½æ•°
            final_pred = np.median(all_predictions, axis=0)

        return final_pred

    def evaluate_all_strategies(self, models, X_train, y_train, X_test, y_test):
        """è¯„ä¼°æ‰€æœ‰é›†æˆç­–ç•¥"""
        print("\nè¯„ä¼°æ‰€æœ‰é›†æˆç­–ç•¥...")

        strategies = {}

        # ç­–ç•¥1: å®‰å…¨åŠ æƒé›†æˆ
        print("1. å®‰å…¨åŠ æƒé›†æˆ...")
        weighted_pred, weights = self.safe_weighted_ensemble(models, X_train, y_train, X_test)
        weighted_r2 = r2_score(y_test, weighted_pred)
        strategies['weighted'] = (weighted_pred, weighted_r2)
        print(f"   åŠ æƒé›†æˆ RÂ²: {weighted_r2:.4f}")

        # ç­–ç•¥2: KæŠ˜æ··åˆé›†æˆ
        print("2. KæŠ˜æ··åˆé›†æˆ...")
        try:
            blended_pred = self.kfold_blending(models, X_train, y_train, X_test)
            blended_r2 = r2_score(y_test, blended_pred)
            strategies['blended'] = (blended_pred, blended_r2)
            print(f"   æ··åˆé›†æˆ RÂ²: {blended_r2:.4f}")
        except Exception as e:
            print(f"   æ··åˆé›†æˆå¤±è´¥: {e}")
            strategies['blended'] = (weighted_pred, weighted_r2)

        # ç­–ç•¥3: æ™ºèƒ½ä¸­ä½æ•°é›†æˆ
        print("3. æ™ºèƒ½ä¸­ä½æ•°é›†æˆ...")
        median_pred = self.smart_median_ensemble(models, X_test, use_weights=True)
        median_r2 = r2_score(y_test, median_pred)
        strategies['median'] = (median_pred, median_r2)
        print(f"   ä¸­ä½æ•°é›†æˆ RÂ²: {median_r2:.4f}")

        # ç­–ç•¥4: æœ€ä½³å•æ¨¡å‹
        print("4. æœ€ä½³å•æ¨¡å‹é€‰æ‹©...")
        best_single_r2 = -np.inf
        best_single_pred = None
        best_single_name = None
        for name, model in models.items():
            pred = model.predict(X_test)
            r2 = r2_score(y_test, pred)
            if r2 > best_single_r2:
                best_single_r2 = r2
                best_single_pred = pred
                best_single_name = name
        strategies['best_single'] = (best_single_pred, best_single_r2)
        print(f"   æœ€ä½³å•æ¨¡å‹ ({best_single_name}) RÂ²: {best_single_r2:.4f}")

        # ç­–ç•¥5: ç®€å•å¹³å‡
        print("5. ç®€å•å¹³å‡é›†æˆ...")
        all_predictions = [model.predict(X_test) for model in models.values()]
        simple_avg_pred = np.mean(all_predictions, axis=0)
        simple_avg_r2 = r2_score(y_test, simple_avg_pred)
        strategies['simple_avg'] = (simple_avg_pred, simple_avg_r2)
        print(f"   ç®€å•å¹³å‡é›†æˆ RÂ²: {simple_avg_r2:.4f}")

        return strategies

    def robust_ensemble_pipeline(self, X_train, y_train, X_test, y_test):
        """é²æ£’é›†æˆç®¡é“"""
        print("å¼€å§‹é²æ£’é›†æˆç®¡é“...")

        # 1. å®‰å…¨ç‰¹å¾å·¥ç¨‹
        print("\n1. å®‰å…¨ç‰¹å¾å·¥ç¨‹...")
        X_train_eng = self.safe_feature_engineering(X_train)
        X_test_eng = self.safe_feature_engineering(X_test)

        # 2. ä¸€è‡´ç‰¹å¾é€‰æ‹©
        print("\n2. ä¸€è‡´ç‰¹å¾é€‰æ‹©...")
        X_train_sel, X_test_sel = self.consistent_feature_selection(
            X_train_eng, y_train, X_test_eng, n_features=400
        )

        # 3. åˆ›å»ºé²æ£’æ¨¡å‹
        print("\n3. åˆ›å»ºé²æ£’æ¨¡å‹...")
        robust_models = self.create_robust_extra_trees(X_train_sel, y_train)

        # 4. è¯„ä¼°æ‰€æœ‰ç­–ç•¥
        print("\n4. è¯„ä¼°é›†æˆç­–ç•¥...")
        all_strategies = self.evaluate_all_strategies(
            robust_models, X_train_sel, y_train, X_test_sel, y_test
        )

        # é€‰æ‹©æœ€ä½³ç­–ç•¥
        best_strategy_name = max(all_strategies.items(), key=lambda x: x[1][1])[0]
        best_pred, best_r2 = all_strategies[best_strategy_name]

        print(f"\n{'=' * 60}")
        print("é›†æˆç­–ç•¥æœ€ç»ˆæ¯”è¾ƒ:")
        for name, (pred, r2) in all_strategies.items():
            print(f"{name:15} | RÂ²: {r2:.4f}")

        print(f"\nğŸ¯ æœ€ä½³ç­–ç•¥: {best_strategy_name}, RÂ²: {best_r2:.4f}")
        print(f"{'=' * 60}")

        return best_pred, best_strategy_name, all_strategies, robust_models


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    print("å¼€å§‹é²æ£’ExtraTreesé›†æˆåˆ†æ...")

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # è®°å½•åŸå§‹æœ€ä½³æ€§èƒ½
    original_best_model = ExtraTreesRegressor(
        n_estimators=300, max_depth=None, random_state=44, n_jobs=-1
    )
    original_best_model.fit(X_train, y_train)
    original_pred = original_best_model.predict(X_test)
    original_r2 = r2_score(y_test, original_pred)
    print(f"åŸå§‹æœ€ä½³æ¨¡å‹RÂ²: {original_r2:.4f}")

    # åˆ›å»ºé²æ£’ä¼˜åŒ–å™¨
    optimizer = RobustExtraTreesOptimizer()

    # æ‰§è¡Œé²æ£’é›†æˆç®¡é“
    final_pred, best_strategy, all_strategies, models = optimizer.robust_ensemble_pipeline(
        X_train, y_train, X_test, y_test
    )

    # è¯„ä¼°æ”¹è¿›
    improvement = all_strategies[best_strategy][1] - original_r2
    print(f"\næ€§èƒ½æ”¹è¿›åˆ†æ:")
    print(f"åŸå§‹RÂ²: {original_r2:.4f}")
    print(f"æ–°RÂ²: {all_strategies[best_strategy][1]:.4f}")
    print(f"æå‡: {improvement:.4f}")

    if improvement > 0:
        print("ğŸ‰ ä¼˜åŒ–æˆåŠŸï¼")
    else:
        print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'çœŸå®å€¼': y_test.values,
        'é¢„æµ‹å€¼': final_pred,
        'æ®‹å·®': y_test.values - final_pred
    })

    results_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\robust_ensemble_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    print(f"\né²æ£’é›†æˆåˆ†æå®Œæˆï¼æœ€ç»ˆRÂ²: {all_strategies[best_strategy][1]:.4f}")


if __name__ == "__main__":
    main()