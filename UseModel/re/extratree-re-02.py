import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')


class ExtraTreesMultiModalEnsemble:
    def __init__(self):
        self.models = {}
        self.performance_metrics = {}

    def create_diverse_extra_trees_models(self, X_train, y_train, n_models=8):
        """åˆ›å»ºå¤šæ ·åŒ–çš„ExtraTreesæ¨¡å‹"""
        print(f"åˆ›å»º {n_models} ä¸ªå¤šæ ·åŒ–çš„ExtraTreesæ¨¡å‹...")

        model_configs = [
            # æ·±åº¦æ ‘é…ç½®
            {
                'name': 'deep_trees',
                'params': {
                    'n_estimators': 500,
                    'max_depth': 35,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 0.9,
                    'bootstrap': True,
                    'random_state': 42
                }
            },
            # æ­£åˆ™åŒ–é…ç½®
            {
                'name': 'regularized',
                'params': {
                    'n_estimators': 400,
                    'max_depth': 20,
                    'min_samples_split': 8,
                    'min_samples_leaf': 3,
                    'max_features': 'sqrt',
                    'bootstrap': True,
                    'random_state': 43
                }
            },
            # å®Œå…¨ç”Ÿé•¿é…ç½®
            {
                'name': 'fully_grown',
                'params': {
                    'n_estimators': 300,
                    'max_depth': None,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': None,
                    'bootstrap': False,
                    'random_state': 44
                }
            },
            # é«˜æ–¹å·®é…ç½®
            {
                'name': 'high_variance',
                'params': {
                    'n_estimators': 600,
                    'max_depth': 25,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 0.6,
                    'bootstrap': True,
                    'random_state': 45
                }
            },
            # ä¿å®ˆé…ç½®
            {
                'name': 'conservative',
                'params': {
                    'n_estimators': 800,
                    'max_depth': 15,
                    'min_samples_split': 15,
                    'min_samples_leaf': 5,
                    'max_features': 0.5,
                    'bootstrap': True,
                    'random_state': 46
                }
            },
            # å¹³è¡¡é…ç½®
            {
                'name': 'balanced',
                'params': {
                    'n_estimators': 350,
                    'max_depth': 28,
                    'min_samples_split': 4,
                    'min_samples_leaf': 2,
                    'max_features': 0.8,
                    'bootstrap': True,
                    'random_state': 47
                }
            },
            # ç‰¹å¾é‡è¦é…ç½®
            {
                'name': 'feature_focused',
                'params': {
                    'n_estimators': 450,
                    'max_depth': 22,
                    'min_samples_split': 3,
                    'min_samples_leaf': 2,
                    'max_features': 0.7,
                    'bootstrap': True,
                    'random_state': 48
                }
            },
            # å¤§æ•°æ®é…ç½®
            {
                'name': 'large_ensemble',
                'params': {
                    'n_estimators': 1000,
                    'max_depth': 18,
                    'min_samples_split': 10,
                    'min_samples_leaf': 4,
                    'max_features': 'log2',
                    'bootstrap': True,
                    'random_state': 49
                }
            }
        ]

        models = {}
        for config in model_configs[:n_models]:
            model = ExtraTreesRegressor(**config['params'], n_jobs=-1)
            model.fit(X_train, y_train)
            models[config['name']] = model

            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            cv_scores = cross_val_score(model, X_train, y_train,
                                        cv=5, scoring='r2', n_jobs=-1)
            mean_r2 = cv_scores.mean()
            std_r2 = cv_scores.std()

            print(f"{config['name']:20} | CV RÂ²: {mean_r2:.4f} (Â±{std_r2:.4f})")

            self.performance_metrics[config['name']] = {
                'mean_r2': mean_r2,
                'std_r2': std_r2,
                'cv_scores': cv_scores
            }

        return models

    def smart_weighted_ensemble(self, models, X_val, y_val, X_test):
        """åŸºäºéªŒè¯é›†æ€§èƒ½çš„æ™ºèƒ½åŠ æƒé›†æˆ"""
        print("è®¡ç®—æ™ºèƒ½æƒé‡...")

        model_weights = {}
        model_performances = {}

        for name, model in models.items():
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            y_pred_val = model.predict(X_val)
            r2_val = r2_score(y_val, y_pred_val)

            # ä½¿ç”¨RÂ²ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ï¼ˆå¤„ç†è´Ÿå€¼ï¼‰
            performance = max(r2_val, 0.01)  # é¿å…é›¶æƒé‡
            model_performances[name] = performance

            # åŸºäºæ€§èƒ½è®¡ç®—æƒé‡ï¼ˆå¯ä»¥å°è¯•ä¸åŒçš„æƒé‡å‡½æ•°ï¼‰
            weight = performance ** 2  # å¹³æ–¹æ”¾å¤§å·®å¼‚
            model_weights[name] = weight

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(model_weights.values())
        for name in model_weights:
            model_weights[name] /= total_weight

        # åŠ æƒé¢„æµ‹
        final_prediction = np.zeros(len(X_test))
        for name, model in models.items():
            pred = model.predict(X_test)
            final_prediction += model_weights[name] * pred

        print("æ¨¡å‹æƒé‡åˆ†é…:")
        for name, weight in model_weights.items():
            print(f"  {name:20} | æƒé‡: {weight:.3f} | RÂ²: {model_performances[name]:.4f}")

        return final_prediction, model_weights

    def create_meta_features_fixed(self, models, X_train, X_test, y_train, n_features_to_keep=50):
        """ä¿®å¤çš„å…ƒç‰¹å¾åˆ›å»ºå‡½æ•°"""
        print("åˆ›å»ºå…ƒç‰¹å¾...")

        # ç¬¬ä¸€å±‚é¢„æµ‹
        train_meta_features = []
        test_meta_features = []

        for name, model in models.items():
            # è®­ç»ƒé›†é¢„æµ‹
            train_pred = model.predict(X_train).reshape(-1, 1)
            train_meta_features.append(train_pred)

            # æµ‹è¯•é›†é¢„æµ‹
            test_pred = model.predict(X_test).reshape(-1, 1)
            test_meta_features.append(test_pred)

        # é€‰æ‹©æœ€é‡è¦çš„åŸå§‹ç‰¹å¾ï¼ˆé¿å…ç»´åº¦çˆ†ç‚¸ï¼‰
        base_model = ExtraTreesRegressor(n_estimators=100, random_state=42)
        base_model.fit(X_train, y_train)
        feature_importance = base_model.feature_importances_
        top_feature_indices = np.argsort(feature_importance)[-n_features_to_keep:]

        # æ·»åŠ æœ€é‡è¦çš„åŸå§‹ç‰¹å¾
        train_original_features = X_train.iloc[:, top_feature_indices].values
        test_original_features = X_test.iloc[:, top_feature_indices].values

        train_meta_features.append(train_original_features)
        test_meta_features.append(test_original_features)

        # å †å ç‰¹å¾
        X_train_meta = np.column_stack(train_meta_features)
        X_test_meta = np.column_stack(test_meta_features)

        print(f"å…ƒç‰¹å¾ç»´åº¦ - è®­ç»ƒé›†: {X_train_meta.shape}, æµ‹è¯•é›†: {X_test_meta.shape}")

        return X_train_meta, X_test_meta

    def stacking_ensemble_fixed(self, base_models, X_train, y_train, X_test):
        """ä¿®å¤çš„å †å é›†æˆ"""
        print("æ„å»ºå †å é›†æˆ...")

        from sklearn.linear_model import Ridge
        from sklearn.ensemble import RandomForestRegressor

        # åˆ›å»ºå…ƒç‰¹å¾
        X_train_meta, X_test_meta = self.create_meta_features_fixed(base_models, X_train, X_test, y_train)

        # å°è¯•å¤šä¸ªå…ƒå­¦ä¹ å™¨
        meta_learners = {
            'ridge': Ridge(alpha=1.0),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        }

        best_meta_pred = None
        best_meta_score = -np.inf
        best_meta_name = ""

        for name, meta_learner in meta_learners.items():
            try:
                # äº¤å‰éªŒè¯è¯„ä¼°å…ƒå­¦ä¹ å™¨
                cv_scores = cross_val_score(meta_learner, X_train_meta, y_train,
                                            cv=3, scoring='r2', n_jobs=-1)
                mean_score = cv_scores.mean()

                print(f"å…ƒå­¦ä¹ å™¨ {name}: CV RÂ² = {mean_score:.4f}")

                if mean_score > best_meta_score:
                    best_meta_score = mean_score
                    meta_learner.fit(X_train_meta, y_train)
                    best_meta_pred = meta_learner.predict(X_test_meta)
                    best_meta_name = name
            except Exception as e:
                print(f"å…ƒå­¦ä¹ å™¨ {name} è®­ç»ƒå¤±è´¥: {e}")
                continue

        print(f"æœ€ä½³å…ƒå­¦ä¹ å™¨: {best_meta_name}, CV RÂ²: {best_meta_score:.4f}")

        return best_meta_pred

    def median_ensemble(self, models, X_test):
        """ä¸­ä½æ•°é›†æˆï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰"""
        print("æ‰§è¡Œä¸­ä½æ•°é›†æˆ...")

        all_predictions = []
        for name, model in models.items():
            pred = model.predict(X_test)
            all_predictions.append(pred)

        # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„
        all_predictions = np.array(all_predictions)

        # è®¡ç®—ä¸­ä½æ•°
        median_pred = np.median(all_predictions, axis=0)

        return median_pred

    def best_model_selection(self, models, X_val, y_val, X_test):
        """é€‰æ‹©æœ€ä½³å•æ¨¡å‹"""
        print("é€‰æ‹©æœ€ä½³å•æ¨¡å‹...")

        best_model_name = None
        best_model_score = -np.inf
        best_prediction = None

        for name, model in models.items():
            y_pred_val = model.predict(X_val)
            r2_val = r2_score(y_val, y_pred_val)

            if r2_val > best_model_score:
                best_model_score = r2_val
                best_model_name = name
                best_prediction = model.predict(X_test)

        print(f"æœ€ä½³å•æ¨¡å‹: {best_model_name}, RÂ²: {best_model_score:.4f}")

        return best_prediction, best_model_name

    def hybrid_ensemble_fixed(self, models, X_train, y_train, X_test, y_test):
        """ä¿®å¤çš„æ··åˆé›†æˆç­–ç•¥"""
        print("æ‰§è¡Œæ··åˆé›†æˆç­–ç•¥...")

        # åˆ†å‰²è®­ç»ƒé›†åˆ›å»ºéªŒè¯é›†
        X_tr, X_val, y_tr, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42
        )

        # åœ¨è®­ç»ƒå­é›†ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
        print("åœ¨è®­ç»ƒå­é›†ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹...")
        sub_models = {}
        for name, model in models.items():
            # ä½¿ç”¨ç›¸åŒçš„å‚æ•°åˆ›å»ºæ–°æ¨¡å‹
            sub_model = ExtraTreesRegressor(**model.get_params())
            sub_model.fit(X_tr, y_tr)
            sub_models[name] = sub_model

        # ç­–ç•¥1: æ™ºèƒ½åŠ æƒé›†æˆ
        print("\n1. æ™ºèƒ½åŠ æƒé›†æˆ...")
        weighted_pred, weights = self.smart_weighted_ensemble(sub_models, X_val, y_val, X_test)
        weighted_r2 = r2_score(y_test, weighted_pred)
        print(f"åŠ æƒé›†æˆ RÂ²: {weighted_r2:.4f}")

        # ç­–ç•¥2: å †å é›†æˆ
        print("\n2. å †å é›†æˆ...")
        try:
            stacked_pred = self.stacking_ensemble_fixed(models, X_train, y_train, X_test)
            stacked_r2 = r2_score(y_test, stacked_pred)
            print(f"å †å é›†æˆ RÂ²: {stacked_r2:.4f}")
        except Exception as e:
            print(f"å †å é›†æˆå¤±è´¥: {e}")
            stacked_pred = weighted_pred  # ä½¿ç”¨åŠ æƒé›†æˆä½œä¸ºå¤‡é€‰
            stacked_r2 = weighted_r2

        # ç­–ç•¥3: ä¸­ä½æ•°é›†æˆ
        print("\n3. ä¸­ä½æ•°é›†æˆ...")
        median_pred = self.median_ensemble(models, X_test)
        median_r2 = r2_score(y_test, median_pred)
        print(f"ä¸­ä½æ•°é›†æˆ RÂ²: {median_r2:.4f}")

        # ç­–ç•¥4: æœ€ä½³å•æ¨¡å‹
        print("\n4. æœ€ä½³å•æ¨¡å‹é€‰æ‹©...")
        best_single_pred, best_model_name = self.best_model_selection(sub_models, X_val, y_val, X_test)
        best_single_r2 = r2_score(y_test, best_single_pred)
        print(f"æœ€ä½³å•æ¨¡å‹ RÂ²: {best_single_r2:.4f}")

        # ç­–ç•¥5: ç®€å•å¹³å‡
        print("\n5. ç®€å•å¹³å‡é›†æˆ...")
        all_predictions = [model.predict(X_test) for model in models.values()]
        simple_avg_pred = np.mean(all_predictions, axis=0)
        simple_avg_r2 = r2_score(y_test, simple_avg_pred)
        print(f"ç®€å•å¹³å‡é›†æˆ RÂ²: {simple_avg_r2:.4f}")

        # æ”¶é›†æ‰€æœ‰ç­–ç•¥
        strategies = {
            'weighted': (weighted_pred, weighted_r2),
            'stacked': (stacked_pred, stacked_r2),
            'median': (median_pred, median_r2),
            'best_single': (best_single_pred, best_single_r2),
            'simple_avg': (simple_avg_pred, simple_avg_r2)
        }

        # é€‰æ‹©æœ€ä½³ç­–ç•¥
        best_strategy_name = max(strategies.items(), key=lambda x: x[1][1])[0]
        best_strategy_pred, best_strategy_r2 = strategies[best_strategy_name]

        print(f"\n{'=' * 50}")
        print("é›†æˆç­–ç•¥æ¯”è¾ƒ:")
        for name, (pred, r2) in strategies.items():
            print(f"{name:15} | RÂ²: {r2:.4f}")

        print(f"\nğŸ¯ æœ€ä½³é›†æˆç­–ç•¥: {best_strategy_name}, RÂ²: {best_strategy_r2:.4f}")
        print(f"{'=' * 50}")

        return best_strategy_pred, best_strategy_name

    def evaluate_ensemble(self, models, X_test, y_test, ensemble_pred, ensemble_name):
        """è¯„ä¼°é›†æˆæ•ˆæœ"""
        print(f"\n{'=' * 60}")
        print(f"{ensemble_name} é›†æˆè¯„ä¼°ç»“æœ")
        print(f"{'=' * 60}")

        # å•ä¸ªæ¨¡å‹æ€§èƒ½
        print("å•ä¸ªæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½:")
        single_model_r2 = {}
        for name, model in models.items():
            pred = model.predict(X_test)
            r2 = r2_score(y_test, pred)
            single_model_r2[name] = r2
            print(f"{name:20} | RÂ²: {r2:.4f}")

        # é›†æˆæ¨¡å‹æ€§èƒ½
        ensemble_r2 = r2_score(y_test, ensemble_pred)
        ensemble_mse = mean_squared_error(y_test, ensemble_pred)
        ensemble_mae = mean_absolute_error(y_test, ensemble_pred)

        print(f"\né›†æˆæ¨¡å‹æ€§èƒ½:")
        print(f"RÂ²:  {ensemble_r2:.4f}")
        print(f"MSE: {ensemble_mse:.4f}")
        print(f"MAE: {ensemble_mae:.4f}")

        # æå‡åˆ†æ
        best_single_r2 = max(single_model_r2.values())
        improvement = ensemble_r2 - best_single_r2
        print(f"\nç›¸å¯¹äºæœ€ä½³å•æ¨¡å‹æå‡: {improvement:.4f}")

        if improvement > 0:
            print("ğŸ¯ é›†æˆç­–ç•¥æœ‰æ•ˆæå‡äº†æ€§èƒ½ï¼")
        else:
            print("âš ï¸  é›†æˆç­–ç•¥æœªèƒ½æå‡æ€§èƒ½")

        return ensemble_r2, improvement


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    print("å¼€å§‹ExtraTreeså¤šæ€é›†æˆåˆ†æ...")

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # åˆ›å»ºå¤šæ€é›†æˆå™¨
    ensemble = ExtraTreesMultiModalEnsemble()

    # 1. åˆ›å»ºå¤šæ ·åŒ–æ¨¡å‹
    diverse_models = ensemble.create_diverse_extra_trees_models(X_train, y_train, n_models=8)

    # 2. æ‰§è¡Œä¿®å¤çš„æ··åˆé›†æˆ
    final_pred, strategy_name = ensemble.hybrid_ensemble_fixed(diverse_models, X_train, y_train, X_test, y_test)

    # 3. è¯„ä¼°é›†æˆæ•ˆæœ
    ensemble_r2, improvement = ensemble.evaluate_ensemble(diverse_models, X_test, y_test, final_pred, strategy_name)

    # 4. ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'çœŸå®å€¼': y_test.values,
        'é¢„æµ‹å€¼': final_pred,
        'æ®‹å·®': y_test.values - final_pred
    })

    results_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\multimodal_ensemble_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    ensemble_info = {
        'strategy': strategy_name,
        'feature_columns': feature_columns,
        'performance': ensemble_r2,
        'improvement': improvement
    }

    import joblib
    joblib.dump(ensemble_info, r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\multimodal_extra_trees_ensemble.pkl")
    print(f"æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜")

    print(f"\nå¤šæ€é›†æˆåˆ†æå®Œæˆï¼æœ€ç»ˆRÂ²: {ensemble_r2:.4f}")


if __name__ == "__main__":
    main()