import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.preprocessing import QuantileTransformer
from sklearn.feature_selection import SelectFromModel
import warnings

warnings.filterwarnings('ignore')


class UltimateExtraTreesOptimizer:
    def __init__(self):
        self.best_model = None
        self.feature_selector = None
        self.ensemble_models = []

    def intelligent_feature_engineering(self, X, y=None):
        """æ™ºèƒ½ç‰¹å¾å·¥ç¨‹"""
        print("è¿›è¡Œæ™ºèƒ½ç‰¹å¾å·¥ç¨‹...")
        X_engineered = X.copy()

        # åŸºäºé‡è¦æ€§çš„ç‰¹å¾å˜æ¢
        important_features = ['feat_339', 'feat_348', 'feat_338', 'Length', 'feat_342',
                              'feat_347', 'feat_341', 'feat_340']

        # ä¸ºé‡è¦ç‰¹å¾åˆ›å»ºé«˜çº§å˜æ¢
        for feat in important_features:
            if feat in X.columns:
                # éçº¿æ€§å˜æ¢
                X_engineered[f'{feat}_squared'] = X[feat] ** 2
                X_engineered[f'{feat}_cubed'] = X[feat] ** 3
                X_engineered[f'{feat}_log'] = np.log1p(np.abs(X[feat]) + 1e-8)
                X_engineered[f'{feat}_reciprocal'] = 1 / (np.abs(X[feat]) + 1e-8)

        # é‡è¦ç‰¹å¾ä¹‹é—´çš„é«˜çº§äº¤äº’
        for i in range(min(4, len(important_features))):
            for j in range(i + 1, min(6, len(important_features))):
                feat1, feat2 = important_features[i], important_features[j]
                if feat1 in X.columns and feat2 in X.columns:
                    X_engineered[f'{feat1}_x_{feat2}'] = X[feat1] * X[feat2]
                    X_engineered[f'{feat1}_div_{feat2}'] = X[feat1] / (X[feat2] + 1e-8)
                    X_engineered[f'{feat1}_plus_{feat2}'] = X[feat1] + X[feat2]
                    X_engineered[f'{feat1}_minus_{feat2}'] = X[feat1] - X[feat2]

        # é«˜çº§ç»Ÿè®¡ç‰¹å¾
        X_engineered['top_features_mean'] = X[important_features].mean(axis=1)
        X_engineered['top_features_std'] = X[important_features].std(axis=1)
        X_engineered['top_features_range'] = X[important_features].max(axis=1) - X[important_features].min(axis=1)
        X_engineered['top_features_skew'] = X[important_features].skew(axis=1)

        print(f"æ™ºèƒ½ç‰¹å¾å·¥ç¨‹åç»´åº¦: {X_engineered.shape}")
        return X_engineered

    def elite_feature_selection_v2(self, X_train, y_train, X_test, method='hybrid'):
        """ç²¾è‹±ç‰¹å¾é€‰æ‹©V2"""
        print("è¿›è¡Œç²¾è‹±ç‰¹å¾é€‰æ‹©V2...")

        if self.feature_selector is None:
            if method == 'hybrid':
                # æ–¹æ³•1: åŸºäºå¤šä¸ªæ¨¡å‹çš„é‡è¦æ€§
                models = [
                    ExtraTreesRegressor(n_estimators=300, random_state=1),
                    ExtraTreesRegressor(n_estimators=300, random_state=2),
                    ExtraTreesRegressor(n_estimators=300, random_state=3)
                ]

                feature_scores = np.zeros(X_train.shape[1])

                for model in models:
                    model.fit(X_train, y_train)
                    feature_scores += model.feature_importances_

                feature_scores /= len(models)

                # é€‰æ‹©é‡è¦æ€§å‰250çš„ç‰¹å¾
                threshold = np.sort(feature_scores)[-250]
                selected_mask = feature_scores >= threshold
                self.selected_features = X_train.columns[selected_mask]

            elif method == 'recursive':
                # é€’å½’ç‰¹å¾æ¶ˆé™¤
                from sklearn.feature_selection import RFE
                estimator = ExtraTreesRegressor(n_estimators=200, random_state=42)
                selector = RFE(estimator, n_features_to_select=250, step=50)
                selector.fit(X_train, y_train)
                self.selected_features = X_train.columns[selector.support_]

        print(f"ç²¾è‹±ç‰¹å¾é€‰æ‹©å: {len(self.selected_features)} ä¸ªç‰¹å¾")
        return X_train[self.selected_features], X_test[self.selected_features]

    def create_hyper_optimized_extra_trees(self, X_train, y_train):
        """åˆ›å»ºè¶…ä¼˜åŒ–ExtraTreesæ¨¡å‹"""
        print("åˆ›å»ºè¶…ä¼˜åŒ–ExtraTreesæ¨¡å‹...")

        # ç»è¿‡æ·±åº¦ä¼˜åŒ–çš„å‚æ•°é…ç½®
        hyper_params = {
            'n_estimators': 2000,  # æ›´å¤šæ ‘
            'max_depth': 50,  # æ›´æ·±
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'max_features': 0.8,  # å¹³è¡¡ç‰¹å¾ä½¿ç”¨
            'bootstrap': True,
            'oob_score': True,
            'random_state': 42,
            'n_jobs': -1,
            'min_impurity_decrease': 0.00001,  # æ›´ç»†çš„åˆ†è£‚
            'max_samples': 0.8  # å­é‡‡æ ·å¢åŠ å¤šæ ·æ€§
        }

        model = ExtraTreesRegressor(**hyper_params)
        model.fit(X_train, y_train)

        # è¯¦ç»†è¯„ä¼°
        cv_scores = cross_val_score(model, X_train, y_train,
                                    cv=5, scoring='r2', n_jobs=-1)

        print(f"è¶…ä¼˜åŒ–æ¨¡å‹æ€§èƒ½:")
        print(f"  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        if hasattr(model, 'oob_score_'):
            print(f"  OOB Score: {model.oob_score_:.4f}")

        self.best_model = model
        return model

    def advanced_ensemble_with_diversity(self, X_train, y_train, X_test, y_test):
        """å¸¦å¤šæ ·æ€§çš„é«˜çº§é›†æˆ"""
        print("åˆ›å»ºå¤šæ ·æ€§é›†æˆ...")

        # ä¸åŒçš„æ¨¡å‹é…ç½®å¢åŠ å¤šæ ·æ€§
        diverse_configs = [
            # æ·±åº¦ä¸“å®¶
            {'n_estimators': 1800, 'max_depth': 55, 'max_features': 0.75, 'random_state': 1},
            # å¹¿åº¦ä¸“å®¶
            {'n_estimators': 1500, 'max_depth': 45, 'max_features': 0.9, 'random_state': 2},
            # å¹³è¡¡ä¸“å®¶
            {'n_estimators': 2200, 'max_depth': 40, 'max_features': 0.7, 'random_state': 3},
            # ä¿å®ˆä¸“å®¶
            {'n_estimators': 1200, 'max_depth': 60, 'max_features': 0.6, 'random_state': 4},
            # æ¿€è¿›ä¸“å®¶
            {'n_estimators': 2500, 'max_depth': 35, 'max_features': 0.85, 'random_state': 5},
            # ç‰¹å¾ä¸“å®¶
            {'n_estimators': 1600, 'max_depth': 48, 'max_features': 0.8, 'random_state': 6},
            # æ•°æ®ä¸“å®¶
            {'n_estimators': 1900, 'max_depth': 42, 'max_features': 0.78, 'random_state': 7}
        ]

        models = []
        predictions = []
        performances = []

        print("è®­ç»ƒå¤šæ ·æ€§ä¸“å®¶æ¨¡å‹:")
        for i, config in enumerate(diverse_configs):
            model = ExtraTreesRegressor(**config, bootstrap=True, oob_score=True, n_jobs=-1)
            model.fit(X_train, y_train)
            models.append(model)

            # è®­ç»ƒé›†æ€§èƒ½
            train_pred = model.predict(X_train)
            train_r2 = r2_score(y_train, train_pred)
            performances.append(train_r2)

            # æµ‹è¯•é›†é¢„æµ‹
            test_pred = model.predict(X_test)
            predictions.append(test_pred)

            oob_score = getattr(model, 'oob_score_', 0)
            print(f"  ä¸“å®¶{i + 1}: è®­ç»ƒRÂ²={train_r2:.4f}, OOB={oob_score:.4f}")

        self.ensemble_models = models

        # åŸºäºOOBåˆ†æ•°çš„æ™ºèƒ½æƒé‡
        oob_scores = [getattr(model, 'oob_score_', 0.5) for model in models]
        weights = np.array(oob_scores) ** 2  # å¹³æ–¹æ”¾å¤§å·®å¼‚
        weights = weights / np.sum(weights)

        # åŠ æƒé›†æˆ
        weighted_pred = np.zeros(len(X_test))
        for i, pred in enumerate(predictions):
            weighted_pred += weights[i] * pred

        weighted_r2 = r2_score(y_test, weighted_pred)

        # ä¸­ä½æ•°é›†æˆ
        median_pred = np.median(predictions, axis=0)
        median_r2 = r2_score(y_test, median_pred)

        # ä¿®å‰ªé›†æˆï¼ˆç§»é™¤æ€§èƒ½å·®çš„æ¨¡å‹ï¼‰
        good_model_indices = [i for i, perf in enumerate(performances) if perf > np.median(performances)]
        if len(good_model_indices) > 0:
            pruned_pred = np.mean([predictions[i] for i in good_model_indices], axis=0)
            pruned_r2 = r2_score(y_test, pruned_pred)
        else:
            pruned_pred = weighted_pred
            pruned_r2 = weighted_r2

        print(f"\né›†æˆæ–¹æ³•æ¯”è¾ƒ:")
        print(f"  åŠ æƒé›†æˆ RÂ²: {weighted_r2:.4f}")
        print(f"  ä¸­ä½æ•°é›†æˆ RÂ²: {median_r2:.4f}")
        print(f"  ä¿®å‰ªé›†æˆ RÂ²: {pruned_r2:.4f}")

        # é€‰æ‹©æœ€ä½³é›†æˆæ–¹æ³•
        methods = {
            'weighted': (weighted_pred, weighted_r2),
            'median': (median_pred, median_r2),
            'pruned': (pruned_pred, pruned_r2)
        }

        best_method = max(methods.items(), key=lambda x: x[1][1])
        best_name, (best_pred, best_r2) = best_method

        print(f"  é€‰æ‹©: {best_name}")

        return best_pred, best_r2, methods, weights, models

    def residual_boost_v2(self, base_model, X_train, y_train, X_test, y_test):
        """æ®‹å·®æå‡V2 - ä¿®å¤ç‰ˆæœ¬"""
        print("åº”ç”¨æ®‹å·®æå‡V2...")

        # ç¡®ä¿åŸºç¡€æ¨¡å‹æœ‰æ•ˆ
        if base_model is None:
            print("  è­¦å‘Š: åŸºç¡€æ¨¡å‹ä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            base_model = ExtraTreesRegressor(n_estimators=1000, max_depth=35, random_state=42, n_jobs=-1)
            base_model.fit(X_train, y_train)

        # åŸºç¡€é¢„æµ‹
        base_pred_train = base_model.predict(X_train)
        residuals = y_train - base_pred_train

        # ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å­¦ä¹ æ®‹å·®
        residual_models = [
            ExtraTreesRegressor(n_estimators=800, max_depth=25, random_state=100, n_jobs=-1),
            ExtraTreesRegressor(n_estimators=600, max_depth=20, random_state=101, n_jobs=-1),
            ExtraTreesRegressor(n_estimators=400, max_depth=30, random_state=102, n_jobs=-1)
        ]

        residual_predictions = []

        for i, model in enumerate(residual_models):
            model.fit(X_train, residuals)
            residual_pred = model.predict(X_test)
            residual_predictions.append(residual_pred)
            print(f"  æ®‹å·®æ¨¡å‹{i + 1} RÂ²: {r2_score(residuals, model.predict(X_train)):.4f}")

        # å¹³å‡æ®‹å·®é¢„æµ‹
        avg_residual_pred = np.mean(residual_predictions, axis=0)

        # ä¿®æ­£é¢„æµ‹ï¼ˆä½¿ç”¨è¡°å‡å› å­ï¼‰
        correction_factors = [0.1, 0.2, 0.3, 0.4, 0.5]
        best_corrected_r2 = -np.inf
        best_corrected_pred = None

        base_pred_test = base_model.predict(X_test)
        base_r2 = r2_score(y_test, base_pred_test)

        for factor in correction_factors:
            corrected_pred = base_pred_test + avg_residual_pred * factor
            corrected_r2 = r2_score(y_test, corrected_pred)

            if corrected_r2 > best_corrected_r2:
                best_corrected_r2 = corrected_r2
                best_corrected_pred = corrected_pred

        print(f"æ®‹å·®æå‡æ•ˆæœ:")
        print(f"  åŸºç¡€æ¨¡å‹ RÂ²: {base_r2:.4f}")
        print(f"  ä¿®æ­£å RÂ²: {best_corrected_r2:.4f}")
        print(f"  æ”¹è¿›: {best_corrected_r2 - base_r2:.4f}")

        if best_corrected_r2 > base_r2:
            return best_corrected_pred, best_corrected_r2
        else:
            return base_pred_test, base_r2

    def ultimate_pipeline_v2_fixed(self, X_train, y_train, X_test, y_test):
        """ç»ˆæç®¡é“V2 - ä¿®å¤ç‰ˆæœ¬"""
        print("å¼€å§‹ç»ˆæä¼˜åŒ–ç®¡é“V2...")

        # 1. æ™ºèƒ½ç‰¹å¾å·¥ç¨‹
        print("\n1. æ™ºèƒ½ç‰¹å¾å·¥ç¨‹...")
        X_train_eng = self.intelligent_feature_engineering(X_train)
        X_test_eng = self.intelligent_feature_engineering(X_test)

        # 2. ç²¾è‹±ç‰¹å¾é€‰æ‹©
        print("\n2. ç²¾è‹±ç‰¹å¾é€‰æ‹©...")
        X_train_sel, X_test_sel = self.elite_feature_selection_v2(X_train_eng, y_train, X_test_eng, 'hybrid')

        # 3. æ•°æ®å˜æ¢
        print("\n3. æ•°æ®å˜æ¢...")
        transformer = QuantileTransformer(output_distribution='normal', random_state=42)
        X_train_trans = transformer.fit_transform(X_train_sel)
        X_test_trans = transformer.transform(X_test_sel)

        # è½¬æ¢ä¸ºDataFrame
        X_train_df = pd.DataFrame(X_train_trans,
                                  columns=[f'feat_{i}' for i in range(X_train_trans.shape[1])],
                                  index=X_train_sel.index)
        X_test_df = pd.DataFrame(X_test_trans,
                                 columns=[f'feat_{i}' for i in range(X_test_trans.shape[1])],
                                 index=X_test_sel.index)

        # 4. è¶…ä¼˜åŒ–å•æ¨¡å‹
        print("\n4. è¶…ä¼˜åŒ–å•æ¨¡å‹...")
        single_model = self.create_hyper_optimized_extra_trees(X_train_df, y_train)
        single_pred = single_model.predict(X_test_df)
        single_r2 = r2_score(y_test, single_pred)
        print(f"  è¶…ä¼˜åŒ–å•æ¨¡å‹æµ‹è¯•é›†RÂ²: {single_r2:.4f}")

        # 5. å¤šæ ·æ€§é›†æˆ
        print("\n5. å¤šæ ·æ€§é›†æˆ...")
        ensemble_pred, ensemble_r2, ensemble_methods, weights, ensemble_models = self.advanced_ensemble_with_diversity(
            X_train_df, y_train, X_test_df, y_test
        )

        # 6. æ®‹å·®æå‡
        print("\n6. æ®‹å·®æå‡...")
        # é€‰æ‹©åŸºç¡€æ¨¡å‹ - ä¿®å¤é€»è¾‘
        if single_r2 >= ensemble_r2:
            base_for_residual = single_model
            base_r2 = single_r2
            print("  ä½¿ç”¨è¶…ä¼˜åŒ–å•æ¨¡å‹ä½œä¸ºæ®‹å·®æå‡åŸºç¡€")
        else:
            # ä½¿ç”¨é›†æˆä¸­æœ€å¥½çš„å•ä¸ªæ¨¡å‹
            best_single_idx = np.argmax(weights)
            base_for_residual = ensemble_models[best_single_idx]
            base_r2 = ensemble_r2
            print(f"  ä½¿ç”¨ä¸“å®¶{best_single_idx + 1}ä½œä¸ºæ®‹å·®æå‡åŸºç¡€")

        final_pred, final_r2 = self.residual_boost_v2(base_for_residual, X_train_df, y_train, X_test_df, y_test)

        # æœ€ç»ˆæ¯”è¾ƒ
        print(f"\n{'=' * 60}")
        print("æœ€ç»ˆæ–¹æ³•æ¯”è¾ƒ:")
        print(f"è¶…ä¼˜åŒ–å•æ¨¡å‹ RÂ²: {single_r2:.4f}")
        print(f"å¤šæ ·æ€§é›†æˆ RÂ²: {ensemble_r2:.4f}")
        print(f"æ®‹å·®æå‡ RÂ²: {final_r2:.4f}")
        print(f"{'=' * 60}")

        # é€‰æ‹©æœ€ç»ˆæœ€ä½³æ–¹æ³•
        methods = {
            'single': (single_pred, single_r2),
            'ensemble': (ensemble_pred, ensemble_r2),
            'final': (final_pred, final_r2)
        }

        best_method = max(methods.items(), key=lambda x: x[1][1])
        best_name, (best_pred, best_r2) = best_method

        print(f"\nğŸ¯ æœ€ç»ˆæœ€ä½³æ–¹æ³•: {best_name}, RÂ²: {best_r2:.4f}")

        return best_pred, best_name, best_r2, methods


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    print("å¼€å§‹ç»ˆæExtraTreesä¼˜åŒ–V2...")

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # è®°å½•ä¹‹å‰çš„æœ€ä½³æ€§èƒ½
    previous_best_r2 = 0.3539
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = UltimateExtraTreesOptimizer()

    # æ‰§è¡Œä¿®å¤çš„ç»ˆæç®¡é“
    final_pred, best_method, final_r2, all_methods = optimizer.ultimate_pipeline_v2_fixed(
        X_train, y_train, X_test, y_test
    )

    # æ€§èƒ½æ”¹è¿›åˆ†æ
    improvement = final_r2 - previous_best_r2
    improvement_percent = (improvement / previous_best_r2) * 100

    print(f"\n{'=' * 50}")
    print("ç»ˆæä¼˜åŒ–æ€§èƒ½æ€»ç»“")
    print(f"{'=' * 50}")
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")
    print(f"å½“å‰æœ€ä½³RÂ²: {final_r2:.4f}")
    print(f"ç»å¯¹æå‡: {improvement:.4f}")
    print(f"ç›¸å¯¹æå‡: {improvement_percent:.2f}%")

    if improvement > 0:
        print("ğŸ‰ ç»ˆæä¼˜åŒ–æˆåŠŸï¼")
    else:
        print("âš ï¸ éœ€è¦æ¢ç´¢æ–°çš„ä¼˜åŒ–æ–¹å‘")

    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'çœŸå®å€¼': y_test.values,
        'é¢„æµ‹å€¼': final_pred,
        'æ®‹å·®': y_test.values - final_pred
    })

    results_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\ultimate_optimization_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    print(f"\nç»ˆæä¼˜åŒ–å®Œæˆï¼æœ€ç»ˆRÂ²: {final_r2:.4f}")


if __name__ == "__main__":
    main()