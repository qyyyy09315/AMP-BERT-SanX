import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
import warnings

warnings.filterwarnings('ignore')


class NeuralExtraTreesHybrid:
    def __init__(self):
        self.et_model = None
        self.nn_model = None
        self.scaler = None
        self.feature_selector = None
        self.selected_features = None
        self.blend_weights = None

    def prepare_features(self, X_train, y_train, X_test, n_features=300):
        """ç‰¹å¾å‡†å¤‡ - ä¿®å¤ç‰ˆæœ¬"""
        print("å‡†å¤‡æ··åˆæ¨¡å‹ç‰¹å¾...")

        # åªåœ¨è®­ç»ƒæ—¶åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
        if self.feature_selector is None:
            selector = SelectFromModel(
                ExtraTreesRegressor(n_estimators=200, random_state=42),
                max_features=n_features,
                threshold=-np.inf
            )
            selector.fit(X_train, y_train)
            self.feature_selector = selector
            self.selected_features = X_train.columns[selector.get_support()]
            print(f"é€‰æ‹©ç‰¹å¾æ•°é‡: {len(self.selected_features)}")

        # ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©å™¨å¤„ç†æ‰€æœ‰æ•°æ®
        X_train_sel = self.feature_selector.transform(X_train)
        X_test_sel = self.feature_selector.transform(X_test)

        # è½¬æ¢ä¸ºDataFrameä¿æŒä¸€è‡´æ€§
        X_train_df = pd.DataFrame(X_train_sel,
                                  columns=[f'selected_feat_{i}' for i in range(X_train_sel.shape[1])],
                                  index=X_train.index)
        X_test_df = pd.DataFrame(X_test_sel,
                                 columns=[f'selected_feat_{i}' for i in range(X_test_sel.shape[1])],
                                 index=X_test.index)

        return X_train_df, X_test_df

    def create_advanced_extra_trees(self, X_train, y_train):
        """åˆ›å»ºé«˜çº§ExtraTreesæ¨¡å‹"""
        print("åˆ›å»ºé«˜çº§ExtraTreesæ¨¡å‹...")

        et_params = {
            'n_estimators': 1200,
            'max_depth': 40,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'max_features': 0.85,
            'bootstrap': True,
            'oob_score': True,
            'random_state': 42,
            'n_jobs': -1
        }

        self.et_model = ExtraTreesRegressor(**et_params)
        self.et_model.fit(X_train, y_train)

        cv_scores = cross_val_score(self.et_model, X_train, y_train,
                                    cv=5, scoring='r2', n_jobs=-1)

        print(f"ExtraTreesæ€§èƒ½:")
        print(f"  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        if hasattr(self.et_model, 'oob_score_'):
            print(f"  OOB Score: {self.et_model.oob_score_:.4f}")

        return self.et_model

    def create_advanced_neural_network(self, X_train, y_train):
        """åˆ›å»ºé«˜çº§ç¥ç»ç½‘ç»œ"""
        print("åˆ›å»ºé«˜çº§ç¥ç»ç½‘ç»œæ¨¡å‹...")

        # æ•°æ®æ ‡å‡†åŒ–ï¼ˆç¥ç»ç½‘ç»œéœ€è¦ï¼‰
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)

        # å¤šç§ç¥ç»ç½‘ç»œæ¶æ„å°è¯•
        nn_architectures = [
            {
                'name': 'æ·±å±‚ç½‘ç»œ',
                'params': {
                    'hidden_layer_sizes': (256, 128, 64, 32),
                    'activation': 'relu',
                    'solver': 'adam',
                    'alpha': 0.001,
                    'learning_rate': 'adaptive',
                    'max_iter': 1000,
                    'early_stopping': True,
                    'validation_fraction': 0.1,
                    'n_iter_no_change': 50,
                    'random_state': 42
                }
            },
            {
                'name': 'å®½ç½‘ç»œ',
                'params': {
                    'hidden_layer_sizes': (512, 256),
                    'activation': 'relu',
                    'solver': 'adam',
                    'alpha': 0.0005,
                    'learning_rate': 'adaptive',
                    'max_iter': 800,
                    'early_stopping': True,
                    'random_state': 42
                }
            },
            {
                'name': 'æ®‹å·®é£æ ¼',
                'params': {
                    'hidden_layer_sizes': (128, 128, 128, 128),
                    'activation': 'relu',
                    'solver': 'adam',
                    'alpha': 0.0001,
                    'learning_rate': 'adaptive',
                    'max_iter': 1200,
                    'early_stopping': True,
                    'random_state': 42
                }
            }
        ]

        best_nn = None
        best_nn_score = -np.inf
        best_nn_name = ""

        for arch in nn_architectures:
            print(f"  å°è¯• {arch['name']}...")
            try:
                nn_model = MLPRegressor(**arch['params'])

                # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°
                cv_scores = cross_val_score(nn_model, X_train_scaled, y_train,
                                            cv=3, scoring='r2')
                mean_score = cv_scores.mean()

                print(f"    {arch['name']} CV RÂ²: {mean_score:.4f}")

                if mean_score > best_nn_score:
                    best_nn_score = mean_score
                    best_nn = nn_model
                    best_nn_name = arch['name']

            except Exception as e:
                print(f"    {arch['name']} è®­ç»ƒå¤±è´¥: {e}")
                continue

        if best_nn is not None:
            print(f"  é€‰æ‹©æœ€ä½³ç¥ç»ç½‘ç»œ: {best_nn_name}")
            best_nn.fit(X_train_scaled, y_train)
            self.nn_model = best_nn
        else:
            # é»˜è®¤ç½‘ç»œ
            print("  ä½¿ç”¨é»˜è®¤ç¥ç»ç½‘ç»œ")
            self.nn_model = MLPRegressor(
                hidden_layer_sizes=(256, 128, 64),
                activation='relu',
                solver='adam',
                alpha=0.001,
                random_state=42,
                max_iter=1000,
                early_stopping=True
            )
            self.nn_model.fit(X_train_scaled, y_train)

        return self.nn_model

    def optimize_blending_weights(self, X_train, y_train, X_val, y_val):
        """ä¼˜åŒ–æ··åˆæƒé‡"""
        print("ä¼˜åŒ–æ¨¡å‹æ··åˆæƒé‡...")

        # è·å–ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹
        et_pred_train = self.et_model.predict(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        nn_pred_train = self.nn_model.predict(X_train_scaled)

        et_pred_val = self.et_model.predict(X_val)
        X_val_scaled = self.scaler.transform(X_val)
        nn_pred_val = self.nn_model.predict(X_val_scaled)

        # ç½‘æ ¼æœç´¢æœ€ä½³æƒé‡
        best_weight = 0.5
        best_r2 = -np.inf

        weights = np.arange(0.1, 1.0, 0.05)
        for w in weights:
            blended_pred = w * et_pred_val + (1 - w) * nn_pred_val
            r2 = r2_score(y_val, blended_pred)

            if r2 > best_r2:
                best_r2 = r2
                best_weight = w

        self.blend_weights = {'et': best_weight, 'nn': 1 - best_weight}

        print(f"ä¼˜åŒ–åçš„æ··åˆæƒé‡:")
        print(f"  ExtraTrees: {best_weight:.3f}")
        print(f"  ç¥ç»ç½‘ç»œ: {1 - best_weight:.3f}")
        print(f"  éªŒè¯é›†RÂ²: {best_r2:.4f}")

        return best_weight

    def create_stacked_features(self, X_train, y_train, X_test):
        """åˆ›å»ºå †å ç‰¹å¾"""
        print("åˆ›å»ºå †å ç‰¹å¾...")

        # è·å–åŸºç¡€é¢„æµ‹
        et_pred_train = self.et_model.predict(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        nn_pred_train = self.nn_model.predict(X_train_scaled)

        et_pred_test = self.et_model.predict(X_test)
        X_test_scaled = self.scaler.transform(X_test)
        nn_pred_test = self.nn_model.predict(X_test_scaled)

        # åˆ›å»ºå †å ç‰¹å¾
        X_train_stacked = np.column_stack([
            X_train.values,
            et_pred_train.reshape(-1, 1),
            nn_pred_train.reshape(-1, 1),
            (et_pred_train * nn_pred_train).reshape(-1, 1),  # äº¤äº’é¡¹
            ((et_pred_train + nn_pred_train) / 2).reshape(-1, 1)  # å¹³å‡é¡¹
        ])

        X_test_stacked = np.column_stack([
            X_test.values,
            et_pred_test.reshape(-1, 1),
            nn_pred_test.reshape(-1, 1),
            (et_pred_test * nn_pred_test).reshape(-1, 1),
            ((et_pred_test + nn_pred_test) / 2).reshape(-1, 1)
        ])

        print(f"å †å ç‰¹å¾ç»´åº¦: {X_train_stacked.shape}")

        return X_train_stacked, X_test_stacked

    def train_meta_learner(self, X_train_stacked, y_train, X_test_stacked, y_test):
        """è®­ç»ƒå…ƒå­¦ä¹ å™¨"""
        print("è®­ç»ƒå…ƒå­¦ä¹ å™¨...")

        # ä½¿ç”¨ExtraTreesä½œä¸ºå…ƒå­¦ä¹ å™¨
        meta_learner = ExtraTreesRegressor(
            n_estimators=500,
            max_depth=20,
            random_state=42,
            n_jobs=-1
        )

        meta_learner.fit(X_train_stacked, y_train)
        meta_pred = meta_learner.predict(X_test_stacked)
        meta_r2 = r2_score(y_test, meta_pred)

        print(f"å…ƒå­¦ä¹ å™¨æ€§èƒ½:")
        print(f"  RÂ²: {meta_r2:.4f}")

        return meta_pred, meta_r2, meta_learner

    def simplified_hybrid_pipeline(self, X_train, y_train, X_test, y_test):
        """ç®€åŒ–çš„æ··åˆæ¨¡å‹ç®¡é“"""
        print("å¼€å§‹ç®€åŒ–çš„ç¥ç»ç½‘ç»œ+ExtraTreesæ··åˆå»ºæ¨¡...")

        # 1. ç‰¹å¾å‡†å¤‡
        print("\n1. ç‰¹å¾å‡†å¤‡...")
        X_train_sel, X_test_sel = self.prepare_features(X_train, y_train, X_test, 350)

        # 2. è®­ç»ƒExtraTrees
        print("\n2. è®­ç»ƒExtraTrees...")
        et_model = self.create_advanced_extra_trees(X_train_sel, y_train)
        et_pred_test = et_model.predict(X_test_sel)
        et_r2_test = r2_score(y_test, et_pred_test)
        print(f"  ExtraTreesæµ‹è¯•é›†RÂ²: {et_r2_test:.4f}")

        # 3. è®­ç»ƒç¥ç»ç½‘ç»œ
        print("\n3. è®­ç»ƒç¥ç»ç½‘ç»œ...")
        nn_model = self.create_advanced_neural_network(X_train_sel, y_train)
        X_test_scaled = self.scaler.transform(X_test_sel)
        nn_pred_test = nn_model.predict(X_test_scaled)
        nn_r2_test = r2_score(y_test, nn_pred_test)
        print(f"  ç¥ç»ç½‘ç»œæµ‹è¯•é›†RÂ²: {nn_r2_test:.4f}")

        # 4. å¤šç§æ··åˆç­–ç•¥
        print("\n4. æ··åˆç­–ç•¥æ¯”è¾ƒ...")

        # ç­–ç•¥1: ç®€å•å¹³å‡
        simple_avg_pred = 0.5 * et_pred_test + 0.5 * nn_pred_test
        simple_avg_r2 = r2_score(y_test, simple_avg_pred)
        print(f"  ç®€å•å¹³å‡ RÂ²: {simple_avg_r2:.4f}")

        # ç­–ç•¥2: åŸºäºéªŒè¯é›†çš„ä¼˜åŒ–æƒé‡
        print("  ä¼˜åŒ–æ··åˆæƒé‡...")
        X_tr, X_val, y_tr, y_val = train_test_split(X_train_sel, y_train, test_size=0.2, random_state=42)

        # åœ¨å­é›†ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹ç”¨äºæƒé‡ä¼˜åŒ–
        et_model_val = ExtraTreesRegressor(n_estimators=800, max_depth=35, random_state=42, n_jobs=-1)
        et_model_val.fit(X_tr, y_tr)

        nn_model_val = MLPRegressor(hidden_layer_sizes=(512, 256), random_state=42, max_iter=800)
        scaler_val = StandardScaler()
        X_tr_scaled = scaler_val.fit_transform(X_tr)
        nn_model_val.fit(X_tr_scaled, y_tr)

        # ä¼˜åŒ–æƒé‡
        et_pred_val = et_model_val.predict(X_val)
        X_val_scaled = scaler_val.transform(X_val)
        nn_pred_val = nn_model_val.predict(X_val_scaled)

        best_weight = 0.5
        best_r2_val = -np.inf
        weights = np.arange(0.1, 1.0, 0.05)

        for w in weights:
            blended_pred = w * et_pred_val + (1 - w) * nn_pred_val
            r2 = r2_score(y_val, blended_pred)
            if r2 > best_r2_val:
                best_r2_val = r2
                best_weight = w

        optimized_pred = best_weight * et_pred_test + (1 - best_weight) * nn_pred_test
        optimized_r2 = r2_score(y_test, optimized_pred)
        print(f"  ä¼˜åŒ–æ··åˆ RÂ²: {optimized_r2:.4f} (æƒé‡: {best_weight:.2f})")

        # ç­–ç•¥3: åŠ¨æ€æƒé‡ï¼ˆåŸºäºæ¨¡å‹ç½®ä¿¡åº¦ï¼‰
        print("  å°è¯•åŠ¨æ€æƒé‡...")
        # ä½¿ç”¨é¢„æµ‹æ–¹å·®ä½œä¸ºç½®ä¿¡åº¦æŒ‡æ ‡
        et_confidence = 1.0 / (1.0 + np.std(et_pred_test))
        nn_confidence = 1.0 / (1.0 + np.std(nn_pred_test))

        dynamic_weights = np.array([et_confidence, nn_confidence])
        dynamic_weights = dynamic_weights / np.sum(dynamic_weights)

        dynamic_pred = dynamic_weights[0] * et_pred_test + dynamic_weights[1] * nn_pred_test
        dynamic_r2 = r2_score(y_test, dynamic_pred)
        print(f"  åŠ¨æ€æƒé‡ RÂ²: {dynamic_r2:.4f} (æƒé‡: {dynamic_weights[0]:.2f}, {dynamic_weights[1]:.2f})")

        # ç­–ç•¥4: é€‰æ‹©æœ€ä½³å•æ¨¡å‹
        single_models = {
            'et': (et_pred_test, et_r2_test),
            'nn': (nn_pred_test, nn_r2_test)
        }
        best_single = max(single_models.items(), key=lambda x: x[1][1])

        # ç»“æœæ¯”è¾ƒ
        methods = {
            'et_only': (et_pred_test, et_r2_test),
            'nn_only': (nn_pred_test, nn_r2_test),
            'simple_avg': (simple_avg_pred, simple_avg_r2),
            'optimized_blend': (optimized_pred, optimized_r2),
            'dynamic_blend': (dynamic_pred, dynamic_r2),
            'best_single': best_single[1]
        }

        print(f"\n{'=' * 60}")
        print("æ··åˆæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
        print(f"{'=' * 60}")
        for name, (pred, r2) in methods.items():
            print(f"{name:15} | RÂ²: {r2:.4f}")

        # é€‰æ‹©æœ€ä½³æ–¹æ³•
        best_method = max(methods.items(), key=lambda x: x[1][1])
        best_name, (best_pred, best_r2) = best_method

        print(f"{'=' * 60}")
        print(f"\nğŸ¯ æœ€ä½³æ··åˆæ–¹æ³•: {best_name}, RÂ²: {best_r2:.4f}")

        return best_pred, best_name, best_r2, methods


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    train_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\train_re_with_delete.csv"
    test_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\test_re_with_delete.csv"

    print("å¼€å§‹ç¥ç»ç½‘ç»œ+ExtraTreesæ··åˆæ¨¡å‹åˆ†æ...")

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in train_df.columns if col != 'value']
    X_train = train_df[feature_columns]
    y_train = train_df['value']
    X_test = test_df[feature_columns]
    y_test = test_df['value']

    print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # è®°å½•ä¹‹å‰çš„æœ€ä½³æ€§èƒ½
    previous_best_r2 = 0.3539
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")

    # åˆ›å»ºæ··åˆæ¨¡å‹
    hybrid_model = NeuralExtraTreesHybrid()

    # æ‰§è¡Œç®€åŒ–çš„æ··åˆç®¡é“
    final_pred, best_method, final_r2, all_methods = hybrid_model.simplified_hybrid_pipeline(
        X_train, y_train, X_test, y_test
    )

    # æ€§èƒ½æ”¹è¿›åˆ†æ
    improvement = final_r2 - previous_best_r2
    improvement_percent = (improvement / previous_best_r2) * 100

    print(f"\n{'=' * 50}")
    print("æ··åˆæ¨¡å‹æ€§èƒ½æ”¹è¿›æ€»ç»“")
    print(f"{'=' * 50}")
    print(f"ä¹‹å‰æœ€ä½³RÂ²: {previous_best_r2:.4f}")
    print(f"æ··åˆæ¨¡å‹RÂ²: {final_r2:.4f}")
    print(f"ç»å¯¹æå‡: {improvement:.4f}")
    print(f"ç›¸å¯¹æå‡: {improvement_percent:.2f}%")

    if improvement > 0:
        print("ğŸ‰ æ··åˆæ¨¡å‹ä¼˜åŒ–æˆåŠŸï¼")
    else:
        print("âš ï¸ æ··åˆæ¨¡å‹æœªèƒ½æå‡æ€§èƒ½")

    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'çœŸå®å€¼': y_test.values,
        'é¢„æµ‹å€¼': final_pred,
        'æ®‹å·®': y_test.values - final_pred
    })

    results_path = r"D:\PyProject\25å“è¶Šæ¯å¤§æ•°æ®\data2\hybrid_model_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\næ··åˆæ¨¡å‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    print(f"\nç¥ç»ç½‘ç»œ+ExtraTreesæ··åˆå»ºæ¨¡å®Œæˆï¼æœ€ç»ˆRÂ²: {final_r2:.4f}")


if __name__ == "__main__":
    main()