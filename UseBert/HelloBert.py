import pandas as pd
from transformers import BertTokenizer, BertModel
import torch
import pyarrow as pa
import pyarrow.parquet as pq

# 1. è®¾ç½®è®¾å¤‡ï¼šä¼˜å…ˆä½¿ç”¨ GPU (CUDA)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# æœ¬åœ°æ¨¡å‹è·¯å¾„
model_dir = "./prot_bert_bfd"

# 2. åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained(model_dir)
model = BertModel.from_pretrained(model_dir)
model.to(device)
model.eval()


# 3. è¯»å–åºåˆ—æ–‡ä»¶ - ä¿ç•™æ‰€æœ‰åˆ—
def read_sequences_with_metadata(file_path):
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(file_path)

    print(f"æ–‡ä»¶åˆ—å: {list(df.columns)}")
    print(f"æ–‡ä»¶å½¢çŠ¶: {df.shape}")

    # è¯»å–åºåˆ—åˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰
    sequences = df.iloc[:, 0].dropna().tolist()
    print(f"è¯»å–åºåˆ—åˆ—: '{df.columns[0]}'")
    print(f"å‰3ä¸ªåºåˆ—é¢„è§ˆ: {sequences[:3]}")

    # ä¿ç•™å…¶ä»–åˆ—ï¼ˆLengthå’Œlabelï¼‰
    metadata_columns = df.columns[1:].tolist()  # ä»ç¬¬äºŒåˆ—å¼€å§‹çš„æ‰€æœ‰åˆ—
    metadata_df = df[metadata_columns].copy()

    print(f"ä¿ç•™çš„å…ƒæ•°æ®åˆ—: {metadata_columns}")

    return sequences, metadata_df


# 4. è·å–åµŒå…¥è¡¨ç¤º
def get_protein_embedding(sequence):
    # ç¡®ä¿åºåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
    if isinstance(sequence, (int, float)):
        sequence = str(int(sequence))
    elif not isinstance(sequence, str):
        sequence = str(sequence)

    # å»é™¤å¯èƒ½çš„ç©ºæ ¼
    sequence = sequence.strip()

    # ProtBert è¦æ±‚æ°¨åŸºé…¸ä¹‹é—´æœ‰ç©ºæ ¼
    sequence = " ".join(list(sequence))

    # Tokenize å¹¶ç›´æ¥ç§»åŠ¨åˆ° GPU
    inputs = tokenizer(
        sequence,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024
    )

    # å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ° GPU
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    # å– [CLS] token çš„ embedding
    cls_embedding = outputs.last_hidden_state[:, 0, :]  # (1, hidden_size)

    return cls_embedding.cpu()


# 5. ä¿å­˜ä¸ºParquetæ ¼å¼çš„å‡½æ•°
def save_as_parquet(df, file_path):
    """
    å°†DataFrameä¿å­˜ä¸ºParquetæ ¼å¼
    """
    try:
        # è½¬æ¢DataFrameä¸ºPyArrow Table
        table = pa.Table.from_pandas(df)

        # ä¿å­˜ä¸ºParquetæ–‡ä»¶
        pq.write_table(table, file_path, compression='snappy')

        print(f"âœ… Parquetæ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
        print(f"æ–‡ä»¶å¤§å°: {pd.DataFrame([file_path]).memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB (ä¼°è®¡)")

        # éªŒè¯æ–‡ä»¶
        verify_table = pq.read_table(file_path)
        print(f"éªŒè¯è¯»å–: {verify_table.num_rows} è¡Œ, {verify_table.num_columns} åˆ—")

    except Exception as e:
        print(f"âŒ ä¿å­˜Parquetæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise


# ä¸»ç¨‹åº
if __name__ == "__main__":
    # è¯»å–åºåˆ—å’Œå…ƒæ•°æ®
    sequences, metadata_df = read_sequences_with_metadata("Data/train_cl.csv")

    embeddings = []
    valid_sequences = []
    valid_indices = []  # è®°å½•æœ‰æ•ˆçš„ç´¢å¼•

    print(f"Processing {len(sequences)} sequences...")

    for i, seq in enumerate(sequences):
        # æ£€æŸ¥åºåˆ—æ˜¯å¦æœ‰æ•ˆ
        if pd.isna(seq) or seq == "":
            continue

        # ç¡®ä¿åºåˆ—æ˜¯å­—ç¬¦ä¸²
        if not isinstance(seq, str):
            seq = str(seq)

        # æ£€æŸ¥åºåˆ—é•¿åº¦
        if len(seq) == 0:
            continue

        try:
            emb = get_protein_embedding(seq)
            embeddings.append(emb)
            valid_sequences.append(seq)
            valid_indices.append(i)  # è®°å½•æœ‰æ•ˆåºåˆ—çš„åŸå§‹ç´¢å¼•

            if (i + 1) % 100 == 0:  # æ¯100ä¸ªåºåˆ—æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"[{i + 1}/{len(sequences)}] Sequence: {seq[:10]}... -> Embedding shape: {emb.shape}")

        except Exception as e:
            print(f"[{i + 1}/{len(sequences)}] å¤„ç†åºåˆ—æ—¶å‡ºé”™: {e}")
            continue

    # æ‰€æœ‰ embedding å¤„ç†å®Œæ¯•
    print(f"âœ… Done! Got {len(embeddings)} embeddings.")

    # ä¿å­˜åµŒå…¥å‘é‡
    torch.save(embeddings, "DataAfterBert/train_em_cl.pt")
    print("ğŸ’¾ Embeddings saved to 'train_em_cl.pt'")

    # ä¿å­˜æœ‰æ•ˆçš„åºåˆ—
    with open("DataAfterBert/valid_sequences.txt", "w") as f:
        for seq in valid_sequences:
            f.write(seq + "\n")
    print("ğŸ’¾ Valid sequences saved to 'valid_sequences.txt'")

    # ä¿å­˜åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´ç»“æœ
    print("\nä¿å­˜åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´ç»“æœ...")

    # åªä¿ç•™æœ‰æ•ˆåºåˆ—å¯¹åº”çš„å…ƒæ•°æ®
    valid_metadata_df = metadata_df.iloc[valid_indices].reset_index(drop=True)

    # åˆ›å»ºå®Œæ•´çš„ç»“æœDataFrameï¼ˆä¸åŒ…å«sequenceåˆ—ï¼‰
    result_df = pd.DataFrame()

    # æ·»åŠ å…ƒæ•°æ®åˆ—
    for col in valid_metadata_df.columns:
        result_df[col] = valid_metadata_df[col].values

    # æ·»åŠ åµŒå…¥å‘é‡ä¿¡æ¯
    embeddings_array = torch.cat(embeddings, dim=0).numpy()
    result_df['embedding_vector'] = list(embeddings_array)

    # ä¿å­˜ä¸ºCSVæ ¼å¼
    result_df.to_csv("DataAfterBert/train_data_complete.csv", index=False)
    print("ğŸ’¾ å®Œæ•´æ•°æ®ä¿å­˜åˆ° 'train_data_complete.csv' (ä¸åŒ…å«sequenceåˆ—)")

    # ä¿å­˜ä¸ºParquetæ ¼å¼
    print("\næ­£åœ¨ä¿å­˜ä¸ºParquetæ ¼å¼...")
    parquet_file_path = "DataAfterBert/train_data_complete.parquet"
    save_as_parquet(result_df, parquet_file_path)

    # æ¯”è¾ƒæ–‡ä»¶å¤§å°
    print("\nğŸ“Š æ–‡ä»¶å¤§å°æ¯”è¾ƒ:")
    try:
        import os

        csv_size = os.path.getsize("DataAfterBert/train_data_complete.csv") / 1024 / 1024
        parquet_size = os.path.getsize(parquet_file_path) / 1024 / 1024

        print(f"CSVæ–‡ä»¶å¤§å°: {csv_size:.2f} MB")
        print(f"Parquetæ–‡ä»¶å¤§å°: {parquet_size:.2f} MB")
        print(f"å‹ç¼©æ¯”: {csv_size / parquet_size:.2f}x")

    except Exception as e:
        print(f"æ— æ³•æ¯”è¾ƒæ–‡ä»¶å¤§å°: {e}")

    # æ˜¾ç¤ºç»“æœç»Ÿè®¡
    print(f"\nç»“æœç»Ÿè®¡:")
    print(f"æœ‰æ•ˆåºåˆ—æ•°é‡: {len(valid_sequences)}")
    print(f"ä¿ç•™çš„å…ƒæ•°æ®åˆ—: {list(valid_metadata_df.columns)}")
    print(f"å®Œæ•´æ•°æ®å½¢çŠ¶: {result_df.shape}")
    print(f"åµŒå…¥å‘é‡ç»´åº¦: {embeddings_array.shape[1]}")

    # æ˜¾ç¤ºå‰å‡ è¡Œç»“æœ
    print(f"\nå‰3è¡Œå®Œæ•´æ•°æ®:")
    print(result_df.head(3))

    # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
    print(f"\nå†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
    memory_usage = result_df.memory_usage(deep=True)
    print(f"æ€»å†…å­˜ä½¿ç”¨: {memory_usage.sum() / 1024 / 1024:.2f} MB")
    for col, usage in memory_usage.items():
        print(f"  {col}: {usage / 1024 / 1024:.2f} MB")

    print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆ!")
    print("å¯ç”¨æ–‡ä»¶æ ¼å¼:")
    print("  - CSV: DataAfterBert/train_data_complete.csv")
    print("  - Parquet: DataAfterBert/train_data_complete.parquet")
